{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m1_Txof1VVTl",
        "XPkZd9PH_uVI",
        "0-qJvLLBU_iX"
      ],
      "authorship_tag": "ABX9TyNIiKY0Ysck0fyREuQNmR6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8284ed90792d4cc5a5ee58cf9d7398d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Restart Runtime",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_98a2a1a949e0434eac16562fed3045f4",
            "style": "IPY_MODEL_8cdc90e710c84a8889698a1a5a482b79",
            "tooltip": "Click me"
          }
        },
        "98a2a1a949e0434eac16562fed3045f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cdc90e710c84a8889698a1a5a482b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "036ad38d87bf475fa5ddcb4af224f1bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "warning",
            "description": "Restart Runtime",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_8d1057982ca741e98e335bd4e22ef12b",
            "style": "IPY_MODEL_a78d8fc0f41a44f383e652acc187b24b",
            "tooltip": "Click me"
          }
        },
        "8d1057982ca741e98e335bd4e22ef12b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78d8fc0f41a44f383e652acc187b24b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "66803ea0ce7f4ec68acce88a38225265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_760474961c4d4c1a8658a843c18b6a5b",
              "IPY_MODEL_d594b17f6f064e41bdb06b708f03618a",
              "IPY_MODEL_25654315c18b4b048e52930c863b37fc"
            ],
            "layout": "IPY_MODEL_a743e8a8c58a4b08ab4d2f2b1c52a35c"
          }
        },
        "760474961c4d4c1a8658a843c18b6a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35ec5d3c9d5b4041b6457da8950f520f",
            "placeholder": "​",
            "style": "IPY_MODEL_22160878f01641b5b6ba3b5905b1d4bd",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "d594b17f6f064e41bdb06b708f03618a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbdbaeba33534a3a97bc42f63a53788c",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8bce9d01d7e428a9ee0085db1b74318",
            "value": 1042301
          }
        },
        "25654315c18b4b048e52930c863b37fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_250363ed37f34983af743f12bbad1f41",
            "placeholder": "​",
            "style": "IPY_MODEL_4c8a676e157048df82a10bc645aa09a0",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 8.04MB/s]"
          }
        },
        "a743e8a8c58a4b08ab4d2f2b1c52a35c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35ec5d3c9d5b4041b6457da8950f520f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22160878f01641b5b6ba3b5905b1d4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbdbaeba33534a3a97bc42f63a53788c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8bce9d01d7e428a9ee0085db1b74318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "250363ed37f34983af743f12bbad1f41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8a676e157048df82a10bc645aa09a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c14699b57004bb4bef8a1ec6409dd9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_379044263cde4328a8a89fcbede49c72",
              "IPY_MODEL_bdea3e91c8bb4a8880cc3ae2a436e478",
              "IPY_MODEL_ced0ced344a9419985db85b54917dd39"
            ],
            "layout": "IPY_MODEL_b672dbc9e26049be86a67614cab1e1c5"
          }
        },
        "379044263cde4328a8a89fcbede49c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bec3b920931444e95edf830512846a4",
            "placeholder": "​",
            "style": "IPY_MODEL_562a98607363474ab2f45389a23203f7",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "bdea3e91c8bb4a8880cc3ae2a436e478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6a3a6da1a624a289d19d745736f28c7",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f65ef08744a84b3ca05f2f48355ff1f5",
            "value": 456318
          }
        },
        "ced0ced344a9419985db85b54917dd39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5f2362c8f0b48f0bf1bcba83fd7f709",
            "placeholder": "​",
            "style": "IPY_MODEL_978d968bb3c442bda3b6f36df85b4bca",
            "value": " 456k/456k [00:00&lt;00:00, 9.29MB/s]"
          }
        },
        "b672dbc9e26049be86a67614cab1e1c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bec3b920931444e95edf830512846a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "562a98607363474ab2f45389a23203f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a3a6da1a624a289d19d745736f28c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f65ef08744a84b3ca05f2f48355ff1f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c5f2362c8f0b48f0bf1bcba83fd7f709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978d968bb3c442bda3b6f36df85b4bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4574a75d6d24f70886e080f4ceec26d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_169284926a014130a103a36d45512a02",
              "IPY_MODEL_ecd4aaceec0c4de88c2ac2030e10a18f",
              "IPY_MODEL_5551941125634c66a847c252d6b33598"
            ],
            "layout": "IPY_MODEL_1ed7bbad325a47268a989f45e11715ce"
          }
        },
        "169284926a014130a103a36d45512a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac574835b2384a448653b9402463474b",
            "placeholder": "​",
            "style": "IPY_MODEL_3f5f635fb96641f59e1a3ea5c2aaf679",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "ecd4aaceec0c4de88c2ac2030e10a18f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b75c72d653e4e59a771d72420cd118a",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2071a9c6066420f90cc78bfb5cb6a0d",
            "value": 1355256
          }
        },
        "5551941125634c66a847c252d6b33598": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb9c994c277454ea892271f47c34f92",
            "placeholder": "​",
            "style": "IPY_MODEL_ab7eeed594f74b6781794272d7bf0cc5",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 16.2MB/s]"
          }
        },
        "1ed7bbad325a47268a989f45e11715ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac574835b2384a448653b9402463474b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5f635fb96641f59e1a3ea5c2aaf679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b75c72d653e4e59a771d72420cd118a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2071a9c6066420f90cc78bfb5cb6a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4eb9c994c277454ea892271f47c34f92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7eeed594f74b6781794272d7bf0cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b8a7181194847828effdbbc51095e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a69cf46858b41f5b4e7f8336b4a84b2",
              "IPY_MODEL_1d18099087d6433baeb52552345984dc",
              "IPY_MODEL_0f44ff721fe643079ef043f93f34e37e"
            ],
            "layout": "IPY_MODEL_0c355bf574074dfaa8c395702e612415"
          }
        },
        "1a69cf46858b41f5b4e7f8336b4a84b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c3eedfd41242af8ad1e312b94f6369",
            "placeholder": "​",
            "style": "IPY_MODEL_45f85312ac1d4bd6a05c3f94a0f9146f",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "1d18099087d6433baeb52552345984dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a7702dea714bd8a8d4482bd376e5aa",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6df82d349b9f46beb665e7bc7df5510e",
            "value": 665
          }
        },
        "0f44ff721fe643079ef043f93f34e37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b59dae32aae4cf3b9d167d0ef71eb49",
            "placeholder": "​",
            "style": "IPY_MODEL_c6799d390e0d498c84be8143ddcae823",
            "value": " 665/665 [00:00&lt;00:00, 36.0kB/s]"
          }
        },
        "0c355bf574074dfaa8c395702e612415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c3eedfd41242af8ad1e312b94f6369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f85312ac1d4bd6a05c3f94a0f9146f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a7702dea714bd8a8d4482bd376e5aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6df82d349b9f46beb665e7bc7df5510e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b59dae32aae4cf3b9d167d0ef71eb49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6799d390e0d498c84be8143ddcae823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "876a0578339847918b25f9400b5c5846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "TEXT:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_1eaa176a2f7346908488860c929df152",
            "placeholder": "Paste here the text you want to summarize",
            "rows": 20,
            "style": "IPY_MODEL_9537cb85b7a24257b8dbe4a222502c7c",
            "value": "Leveraging LLMs on your domain-specific knowledge base\nWith RAG to Riches: wielding the power of LLMs using Retrieval-Augmented Generation to talk to your data\nAuthor: Michiel De Koninck\n\n\nAsk ChatGPT a question about the origin of the word “marathon” and it will accurately tell you how Herodotus described the legendary 42km run that Pheidippides completed from Marathon to Athens before collapsing from exhaustion.\n\nBut what about my grandmother’s list of recipes? Sure, I can digitise those recipes, no problem. But what if I want to have advice on what meal to prepare based on the ingredients in my fridge, my favourite color and my mood for the day?\n\nLet’s see if that’s possible without collapsing from exhaustion.\n\nLLMs, meet your limits … and exceed them\n\nAn LLM is a Large Language Model. OpenAI’s GPT-4 is one example, Meta’s LLamA is another. We make the conscious choice here to stick to the general LLM term to refer to these models. Bear in mind: each of these models were trained on a gigantic set of (publicly available) data.\n\nIt has been clearly demonstrated by now that these LLMs have a meaningful understanding of general language and that they are able to (re)produce information relevant to the information that was present in their training data. This is why generative tools like ChatGPT perform astonishingly well at answering questions about topics that the LLM encountered during its training.\n\nBut what remains out of the direct grasp of those massive LLMs is the data that is so valuable within each organisation: the internal knowledge base. The question that thus massively pops up is:\n\nHow can we leverage the power of these LLMs in unlocking information stored in a specific knowledge base upon which it wasn’t originally trained?\n\nOh okay, so to do this, can’t we just introduce our internal knowledge base as extra data upon which the LLM should be trained? Or, if you will, can we fine-tune the LLM on our specific knowledge base.\n\nYes, you most likely can. But for reliable question answering, it might not be the way to go.\n\nWhy fine-tuning won’t always cut it\n\nMeet Billy the Bookworm. Billy is a large language model and he has devoured a gigantic amount of online information, empowering with enormous knowledge. Billy however, smart as he is, has not read through the books in your very specific home library.\n\nFine-tuning is this: presenting Billy the Bookworm with all the books in your very specific knowledge base and letting him gobble up all that tasty extra information. This way, the LLM bookworm Billy doesn’t just know all of that general information, he also “knows” a lot about the contents of your specific knowledge base.\n\nClassical approach of fine-tuning on domain specific data (all icons from flaticon)\n\nCongratulations, through this fine-tuning proces you’ve turned Billy into a very specific Billy that knows so much about your specific domain! Below we show how you could start putting Billy to work. By posing questions to your improved bookworm, you can expect answers that use both the information from its gigantic general training set and information stored in your specific knowledge base.\n\nLeveraging the fine-tuned LLM to ask questions about your internal knowledge base.\n\nWhile certainly powerful, the crucial problem with this solution approach, is that you still have little insights into how your bookworm came up with its answers. Moreover, fine-tuning an LLM has its (costly) consequences.\n\nWe list the main reasons why fine-tuning Billy comes up short:\n\nNo source clarity. It’s difficult to prevent hallucination and your LLM has no clear distinction between “general” and “specific” knowledge.\nNo access restriction. Imagine a case where some users should be able to query the information of strategic documents while others shouldn’t. How would you tackle this? Your fine-tuned Billy just knows everything, he can’t choose to leave out knowledge at inference time.\nHosting an LLM is costly. Once you have a fine-tuned LLM, you have to keep it spinning. A large language model is well… large. Costs to keep it up and running will rack up. Do the benefits outweigh those costs?\nFine-tuning repetitions. Model retraining is required when you want the model to reflect changes to the knowledge base.\n\nLuckily, all these problems are solvable. If answering questions in a verifiable way and preventing hallucination is what you are looking for: you may not need the hyper-modern bookworm, let’s just ask the good old librarian where to find the answers to your questions.\n\nWith RAG to Riches\n\nThe idea behind Retrieval-Augmented Generation (RAG) is quite straight-forward. Remember, the goal is to unlock the information in our knowledge base. Instead of unleashing (i.e. fine-tuning) our bookworm on it, we comprehensively index the information of our knowledge base.\n\nBy indexing the embeddings of your internal knowledge base, you unlock smart search capabilities.\n\nIn the schema above, we illustrate how the Smart Retriever functions like a librarian. Ideally, the librarian has perfect knowledge of what is in his library. For a visitor asking a certain question, he would know just which chapter of which book to recommend.\n\nOn a more technical level, this describes a semantic search engine. In this case, the embeddings are vectorial representations of document sections and they allow a mathematical description of the actual meaning stored in each section. By comparing embeddings, we can determine which text sections are similar in meaning to which other text sections. This is crucial for the retrieval process displayed below.\n\nThrough leveraging our Smart Retriever, we can force our generator to stick to the content of our knowledge base that is most relevant for answering the question. Et voilà: Retrieval-Augmented Generation.\n\nIn play are two crucial components:\n\nThe Smart Retriever (i.e. the librarian)\nThe Generator (i.e. the bookworm)\n\nIt should be clear by now why this approach is called Retrieval-Augmented Generation. Based on the question asked, you first retrieve the most relevant information from your internal knowledge base; you then augment the typical generation phase by passing that relevant information explicitly to the generator component.\n\nKey highlights of this RAG-based setup\nClear indication of the source upon which the answer was based. Allowing for validation of the answer returned by the generator.\nVery unlikely to hallucinate, by restricting our generator component to the corpus of our knowledge base, it will admit it can’t formulate a response when no relevant sources were found by the retriever.\nMaintainable Search Index. A knowledge base is a living thing, when it changes, we can adapt our Search Index to reflect those changes.\n\nAside from those highlights, the multi-lingual aspect of LLMs is a thing of beauty. You can have a knowledge base consisting of purely Italian recipes which your pasta-loving French friend can talk to in an all-French dialogue.\n\nFine-tuning revisited\n\nNote that in the section above, we dismissed fine-tuning as a valuable option because we had little control on source clarity thus increasing the risk for hallucination.\n\nIt must be noted that the RAG approach, powered by a general LLM, only works well as long as the specific knowledge base does not contain super specific jargon that the LLM can’t understand from its general training.\n\nImagine you need the responses of your solution to follow ‘the tone and lingo’ that is present in your knowledge base. In this case the fine-tuning of your LLM seems less avoidable.\n\nIt could be a valid approach to be able to handle specific jargon and then incorporate your fine-tuned LLM in the RAG architecture to reap the combined benefits. Instead of working with a general bookworm, you would then use your specifically trained Billy to power the Generator and/or the Smart Retriever components.\n\nWhy now? What’s new?\n\nExcellent question.\nSemantic Search (smart retrieval) has been around for quite some time and so has generative AI (some primitive forms have been around for decades).\nHowever, we have seen pivotal advancements over the last months.\n\nOn a technological level, we’ve recently witnessed big leaps forward in LLM performance. These positively impact the RAG solution on two levels:\n\nEmbeddings (e.g. Embedding API by OpenAI or Google’s PaLM)\nGenerative capabilities (e.g. OpenAI’s ChatGPT solution)\n\nAccompanying that improved generative quality is the increase in traction. Previously, companies could not easily imagine the opportunities of a system relying on generative AI. Now however, thanks to the wide media coverage and adoption of tools like ChatGPT, overall interest has grown exponentially.\n\nSo, though arguably mediocre versions of RAG might have been possible for quite some time, technological improvements and increased traction result in a fruitful market opportunity.\n\nChallenges on your way to success\n\nIn this section, we aim to introduce you to some of the main challenges with setting up a successful RAG solution.\n\nStrong dependency on the performance of the Smart Retriever.\nThe quality of the responses given by your Generative Component will depend directly on the relevancy of the information handed to it by the Smart Retriever. As mentioned above, we can thank LLM advancements for giving us rich and powerful text embeddings. But fetching these embeddings purely via API’s may not be your best option. You should be very conscious when designing your Semantic Search component, perhaps your knowledge base has specific jargon and you might need a custom fitted (i.e. fine-tuned) component to handle it. A more in-depth practical guide on Semantic Search can be found in this blogpost [1] .\nTrade-off to be made in restriction to stick to info in knowledge base.\nAs explained in the RAG architecture, we can force our LLM generative component to restrict itself to the information found in the relevant documents. While this ensures that hallucination (i.e. non-sensical answers) has little chance, it also means you are barely leveraging the information your LLM possesses. You might want your solution to use that knowledge as well but maybe only when requested by the user.\nConversational design to allow complex dialogue.\nWhile our depictions above have represented the user behaviour as asking merely a “one-shot question”, often your user might want to zoom in on the answer provided by your solution (in a ChatGPT-style conversation). Luckily, tools exist to aid you in this battle. The langchain framework offers a helping hand in getting this just right.\nPrompt engineering as a way to steer generation toward succes.\nTo get the answer of your generative component just right, you need to tell it exactly what kind of output you expect. Overall, this is far from rocket science. But getting your prompt setup just right for your use case takes time and deserves enough attention. It may be worthwhile looking at prompt management systems to make sure you can keep track of which prompting works best for which situations.\nChoosing the right LLM: what does it cost and where does my data go?\nThroughout this text, we haven’t made any explicit choice regarding what LLM(s) to use in your solution. When choosing which LLM (API) to use, make sure to take privacy and cost restrictions into consideration. There are quite some decent options out there already. We have OpenAI’s GPT, Meta’s LLaMA, Google’s PaLM and with Elon Musk claiming to join the LLM scene, who knows where things will go. The exciting news is: more options will come and competition should drive LLM performance up and prices down.\nGetting and keeping your LLM solution in production (LLMOps).\nAs with all mature AI solutions: building them is one thing, getting/keeping them in production is another. The field of LLMOps focusses on the operationalisation of LLMs. Monitoring the performance of your LLM-based solution, keeping your knowledge base and search index up-to-date, processing conversational history…\nBefore flinging your LLM solution into production, think wisely about how to maintain it and how to keep it fruitful in the long run.\n\nCharmed by the potential of RAG and intrigued by the related challenges, we now move on to looking at an actual RAG-based solution.\n\nGetting your hands dirty with RAG\n\nIf your interest is sparked by the concept of Retrieval-Augmented Generation, you may be asking yourself:\n\nDo I have what it takes to take a RAG-based solution for a spin?\n\nWell, if you have:\n\nspecific knowledge: a moderate (preferably organised) database of “knowledge articles” that contain useful information that is not easily found on the world-wide web (e.g. technical documents, onboarding guidelines, handled support tickets…)\nbusiness value: a clear definition of business value if that information could be unlocked for the intended users\n\nThen yes, RAG might be the way to go for you.\n\nAs an experiment, we recently built a small demo to showcase how this technology can be leveraged to support government staff in answering parliamentary questions more easily.\nIn this case, the specific knowledge consists of:\n\na set of Flemish legislative documents\na set of parliamentary questions of the past\n\nThe business value, meanwhile is in:\n\nimproving efficiency by automatically suggesting answers to parliamentary questions based on the Flemish knowledge base\nimproving transparency and user adoption through explicit citations\nScreenshot of demo solution built around the case of “application to help answer parliamentary questions”"
          }
        },
        "1eaa176a2f7346908488860c929df152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "75%"
          }
        },
        "9537cb85b7a24257b8dbe4a222502c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68b523f0309a47a889f109066eaf22a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Start Summarization",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_c2ce562277d440ce932aecc3e2fde479",
            "style": "IPY_MODEL_66231ddcd15142e384edf76322ae9c86",
            "tooltip": "Click me"
          }
        },
        "c2ce562277d440ce932aecc3e2fde479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66231ddcd15142e384edf76322ae9c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3c135f52d2884d4a961eacb375ad0738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "TEXT:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_55b2e5aae67c4d27add54f00722df8de",
            "placeholder": "Paste here the text you want to summarize",
            "rows": 20,
            "style": "IPY_MODEL_cb55b792c7e348989f9efa4056a5b74a",
            "value": "Title: Leveraging LLMs on your domain-specific knowledge base | by Michiel De Koninck | May, 2023 | ML6team\n\nLeveraging LLMs on your domain-specific knowledge base\nWith RAG to Riches: wielding the power of LLMs using Retrieval-Augmented Generation to talk to your data\nAuthor: Michiel De Koninck\n\n\nAsk ChatGPT a question about the origin of the word “marathon” and it will accurately tell you how Herodotus described the legendary 42km run that Pheidippides completed from Marathon to Athens before collapsing from exhaustion.\n\nBut what about my grandmother’s list of recipes? Sure, I can digitise those recipes, no problem. But what if I want to have advice on what meal to prepare based on the ingredients in my fridge, my favourite color and my mood for the day?\n\nLet’s see if that’s possible without collapsing from exhaustion.\n\nLLMs, meet your limits … and exceed them\n\nAn LLM is a Large Language Model. OpenAI’s GPT-4 is one example, Meta’s LLamA is another. We make the conscious choice here to stick to the general LLM term to refer to these models. Bear in mind: each of these models were trained on a gigantic set of (publicly available) data.\n\nIt has been clearly demonstrated by now that these LLMs have a meaningful understanding of general language and that they are able to (re)produce information relevant to the information that was present in their training data. This is why generative tools like ChatGPT perform astonishingly well at answering questions about topics that the LLM encountered during its training.\n\nBut what remains out of the direct grasp of those massive LLMs is the data that is so valuable within each organisation: the internal knowledge base. The question that thus massively pops up is:\n\nHow can we leverage the power of these LLMs in unlocking information stored in a specific knowledge base upon which it wasn’t originally trained?\n\nOh okay, so to do this, can’t we just introduce our internal knowledge base as extra data upon which the LLM should be trained? Or, if you will, can we fine-tune the LLM on our specific knowledge base.\n\nYes, you most likely can. But for reliable question answering, it might not be the way to go.\n\nWhy fine-tuning won’t always cut it\n\nMeet Billy the Bookworm. Billy is a large language model and he has devoured a gigantic amount of online information, empowering with enormous knowledge. Billy however, smart as he is, has not read through the books in your very specific home library.\n\nFine-tuning is this: presenting Billy the Bookworm with all the books in your very specific knowledge base and letting him gobble up all that tasty extra information. This way, the LLM bookworm Billy doesn’t just know all of that general information, he also “knows” a lot about the contents of your specific knowledge base.\n\nClassical approach of fine-tuning on domain specific data (all icons from flaticon)\n\nCongratulations, through this fine-tuning proces you’ve turned Billy into a very specific Billy that knows so much about your specific domain! Below we show how you could start putting Billy to work. By posing questions to your improved bookworm, you can expect answers that use both the information from its gigantic general training set and information stored in your specific knowledge base.\n\nLeveraging the fine-tuned LLM to ask questions about your internal knowledge base.\n\nWhile certainly powerful, the crucial problem with this solution approach, is that you still have little insights into how your bookworm came up with its answers. Moreover, fine-tuning an LLM has its (costly) consequences.\n\nWe list the main reasons why fine-tuning Billy comes up short:\n\nNo source clarity. It’s difficult to prevent hallucination and your LLM has no clear distinction between “general” and “specific” knowledge.\nNo access restriction. Imagine a case where some users should be able to query the information of strategic documents while others shouldn’t. How would you tackle this? Your fine-tuned Billy just knows everything, he can’t choose to leave out knowledge at inference time.\nHosting an LLM is costly. Once you have a fine-tuned LLM, you have to keep it spinning. A large language model is well… large. Costs to keep it up and running will rack up. Do the benefits outweigh those costs?\nFine-tuning repetitions. Model retraining is required when you want the model to reflect changes to the knowledge base.\n\nLuckily, all these problems are solvable. If answering questions in a verifiable way and preventing hallucination is what you are looking for: you may not need the hyper-modern bookworm, let’s just ask the good old librarian where to find the answers to your questions.\n\nWith RAG to Riches\n\nThe idea behind Retrieval-Augmented Generation (RAG) is quite straight-forward. Remember, the goal is to unlock the information in our knowledge base. Instead of unleashing (i.e. fine-tuning) our bookworm on it, we comprehensively index the information of our knowledge base.\n\nBy indexing the embeddings of your internal knowledge base, you unlock smart search capabilities.\n\nIn the schema above, we illustrate how the Smart Retriever functions like a librarian. Ideally, the librarian has perfect knowledge of what is in his library. For a visitor asking a certain question, he would know just which chapter of which book to recommend.\n\nOn a more technical level, this describes a semantic search engine. In this case, the embeddings are vectorial representations of document sections and they allow a mathematical description of the actual meaning stored in each section. By comparing embeddings, we can determine which text sections are similar in meaning to which other text sections. This is crucial for the retrieval process displayed below.\n\nThrough leveraging our Smart Retriever, we can force our generator to stick to the content of our knowledge base that is most relevant for answering the question. Et voilà: Retrieval-Augmented Generation.\n\nIn play are two crucial components:\n\nThe Smart Retriever (i.e. the librarian)\nThe Generator (i.e. the bookworm)\n\nIt should be clear by now why this approach is called Retrieval-Augmented Generation. Based on the question asked, you first retrieve the most relevant information from your internal knowledge base; you then augment the typical generation phase by passing that relevant information explicitly to the generator component.\n\nKey highlights of this RAG-based setup\nClear indication of the source upon which the answer was based. Allowing for validation of the answer returned by the generator.\nVery unlikely to hallucinate, by restricting our generator component to the corpus of our knowledge base, it will admit it can’t formulate a response when no relevant sources were found by the retriever.\nMaintainable Search Index. A knowledge base is a living thing, when it changes, we can adapt our Search Index to reflect those changes.\n\nAside from those highlights, the multi-lingual aspect of LLMs is a thing of beauty. You can have a knowledge base consisting of purely Italian recipes which your pasta-loving French friend can talk to in an all-French dialogue.\n\nFine-tuning revisited\n\nNote that in the section above, we dismissed fine-tuning as a valuable option because we had little control on source clarity thus increasing the risk for hallucination.\n\nIt must be noted that the RAG approach, powered by a general LLM, only works well as long as the specific knowledge base does not contain super specific jargon that the LLM can’t understand from its general training.\n\nImagine you need the responses of your solution to follow ‘the tone and lingo’ that is present in your knowledge base. In this case the fine-tuning of your LLM seems less avoidable.\n\nIt could be a valid approach to be able to handle specific jargon and then incorporate your fine-tuned LLM in the RAG architecture to reap the combined benefits. Instead of working with a general bookworm, you would then use your specifically trained Billy to power the Generator and/or the Smart Retriever components.\n\nWhy now? What’s new?\n\nExcellent question.\nSemantic Search (smart retrieval) has been around for quite some time and so has generative AI (some primitive forms have been around for decades).\nHowever, we have seen pivotal advancements over the last months.\n\nOn a technological level, we’ve recently witnessed big leaps forward in LLM performance. These positively impact the RAG solution on two levels:\n\nEmbeddings (e.g. Embedding API by OpenAI or Google’s PaLM)\nGenerative capabilities (e.g. OpenAI’s ChatGPT solution)\n\nAccompanying that improved generative quality is the increase in traction. Previously, companies could not easily imagine the opportunities of a system relying on generative AI. Now however, thanks to the wide media coverage and adoption of tools like ChatGPT, overall interest has grown exponentially.\n\nSo, though arguably mediocre versions of RAG might have been possible for quite some time, technological improvements and increased traction result in a fruitful market opportunity.\n\nChallenges on your way to success\n\nIn this section, we aim to introduce you to some of the main challenges with setting up a successful RAG solution.\n\nStrong dependency on the performance of the Smart Retriever.\nThe quality of the responses given by your Generative Component will depend directly on the relevancy of the information handed to it by the Smart Retriever. As mentioned above, we can thank LLM advancements for giving us rich and powerful text embeddings. But fetching these embeddings purely via API’s may not be your best option. You should be very conscious when designing your Semantic Search component, perhaps your knowledge base has specific jargon and you might need a custom fitted (i.e. fine-tuned) component to handle it. A more in-depth practical guide on Semantic Search can be found in this blogpost [1] .\nTrade-off to be made in restriction to stick to info in knowledge base.\nAs explained in the RAG architecture, we can force our LLM generative component to restrict itself to the information found in the relevant documents. While this ensures that hallucination (i.e. non-sensical answers) has little chance, it also means you are barely leveraging the information your LLM possesses. You might want your solution to use that knowledge as well but maybe only when requested by the user.\nConversational design to allow complex dialogue.\nWhile our depictions above have represented the user behaviour as asking merely a “one-shot question”, often your user might want to zoom in on the answer provided by your solution (in a ChatGPT-style conversation). Luckily, tools exist to aid you in this battle. The langchain framework offers a helping hand in getting this just right.\nPrompt engineering as a way to steer generation toward succes.\nTo get the answer of your generative component just right, you need to tell it exactly what kind of output you expect. Overall, this is far from rocket science. But getting your prompt setup just right for your use case takes time and deserves enough attention. It may be worthwhile looking at prompt management systems to make sure you can keep track of which prompting works best for which situations.\nChoosing the right LLM: what does it cost and where does my data go?\nThroughout this text, we haven’t made any explicit choice regarding what LLM(s) to use in your solution. When choosing which LLM (API) to use, make sure to take privacy and cost restrictions into consideration. There are quite some decent options out there already. We have OpenAI’s GPT, Meta’s LLaMA, Google’s PaLM and with Elon Musk claiming to join the LLM scene, who knows where things will go. The exciting news is: more options will come and competition should drive LLM performance up and prices down.\nGetting and keeping your LLM solution in production (LLMOps).\nAs with all mature AI solutions: building them is one thing, getting/keeping them in production is another. The field of LLMOps focusses on the operationalisation of LLMs. Monitoring the performance of your LLM-based solution, keeping your knowledge base and search index up-to-date, processing conversational history…\nBefore flinging your LLM solution into production, think wisely about how to maintain it and how to keep it fruitful in the long run.\n\nCharmed by the potential of RAG and intrigued by the related challenges, we now move on to looking at an actual RAG-based solution.\n\nGetting your hands dirty with RAG\n\nIf your interest is sparked by the concept of Retrieval-Augmented Generation, you may be asking yourself:\n\nDo I have what it takes to take a RAG-based solution for a spin?\n\nWell, if you have:\n\nspecific knowledge: a moderate (preferably organised) database of “knowledge articles” that contain useful information that is not easily found on the world-wide web (e.g. technical documents, onboarding guidelines, handled support tickets…)\nbusiness value: a clear definition of business value if that information could be unlocked for the intended users\n\nThen yes, RAG might be the way to go for you.\n\nAs an experiment, we recently built a small demo to showcase how this technology can be leveraged to support government staff in answering parliamentary questions more easily.\nIn this case, the specific knowledge consists of:\n\na set of Flemish legislative documents\na set of parliamentary questions of the past\n\nThe business value, meanwhile is in:\n\nimproving efficiency by automatically suggesting answers to parliamentary questions based on the Flemish knowledge base\nimproving transparency and user adoption through explicit citations\nScreenshot of demo solution built around the case of “application to help answer parliamentary questions”\n"
          }
        },
        "55b2e5aae67c4d27add54f00722df8de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "75%"
          }
        },
        "cb55b792c7e348989f9efa4056a5b74a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb98435005ff44ea8027e7579c1415e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "info",
            "description": "Start Summarization",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_e78843edd3af4d7492423c84cb140e87",
            "style": "IPY_MODEL_18cf834295c34157aba19f3169e95f79",
            "tooltip": "Click me"
          }
        },
        "e78843edd3af4d7492423c84cb140e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18cf834295c34157aba19f3169e95f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c9ed1f36b73d4dd69e109f895d64d13d": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_1c9f6a3a048e4e528e240d48107488c2",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m████████████▁▁▁▁▁▁▁▁\u001b[0m Running Summarization pipeline...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">████████████▁▁▁▁▁▁▁▁</span> Running Summarization pipeline...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "1c9f6a3a048e4e528e240d48107488c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6152573912dd4e3eb8f320de8271b981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "danger",
            "description": "Download Summary",
            "disabled": false,
            "icon": "check",
            "layout": "IPY_MODEL_b1a7aa03f79b42eaae388f494c0adbc2",
            "style": "IPY_MODEL_e1264139a204409a923368c7e79c5640",
            "tooltip": "Click me"
          }
        },
        "b1a7aa03f79b42eaae388f494c0adbc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1264139a204409a923368c7e79c5640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b1dcfb1c199d4f92a797d0fd3724ee1e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e15cbded17404d609307ad99a67463a0",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m▐     ⠂  ▌\u001b[0m Running Summarization pipeline...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">▐     ⠂  ▌</span> Running Summarization pipeline...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e15cbded17404d609307ad99a67463a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d58c99cce447f7a16e6fad51fabbf5": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b6a9b60406254d66885a0141dcb1478d",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m▸▹▹▹▹\u001b[0m Running Summarization pipeline...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">▸▹▹▹▹</span> Running Summarization pipeline...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "b6a9b60406254d66885a0141dcb1478d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "714b9fee46ba44e58e1ad1077bff6d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "TEXT:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ea3a996bd5964e638eb55bb23b3161dd",
            "placeholder": "Paste here the text you want to summarize",
            "rows": 20,
            "style": "IPY_MODEL_b339040d015a4eaf8d9abd09d092ab27",
            "value": "Title: Dear Sam Altman- There was never an era of making models bigger \nAuthor: Devansh- Machine Learning Made Simple\n\nRecently, the internet caught fire with a particular admission from Sam Altman- The Era of Large Language Models is over. According to this report by Wired-\n\nBut the company’s CEO, Sam Altman, says further progress will not come from making models bigger. “I think we’re at the end of the era where it’s going to be these, like, giant, giant models,” he told an audience at an event held at MIT late last week. “We’ll make them better in other ways.”\n\n- Article- OpenAI’s CEO Says the Age of Giant AI Models Is Already Over\n\nThis has caused a notable stir in the online space. We have seen a notable increase in the number of AI Experts (specifically GPT Experts) since November 2022. These GPT/LLM warriors have been promising AGI and 100x productivity-based AI based on how adding trillions of parameters and more data might be coming to an end sooner than people realize. Looks like Alex Hormozi can live a peaceful existence now-\n\nIn this article, I’m here to argue something that would be considered blasphemy to many people in the AI Space- the age of mindlessly scaling models has never been here. When we look at the data and actually compare the results- it has always been clear that throwing more and more data and increasing parameter size was always doomed to fail- long before we started hitting the scaling limits that GPT-4 is starting to hit. By understanding how we could have foreseen this problem, we can avoid making these mistakes in the future- saving everyone a lot of time, money, and attention.\n\nOpenAI, the AI research company behind popular language models like ChatGPT and Dall-E 2, has reportedly doubled its losses to $540 million in 2022 due to soaring development expenses for its chatbot. The company is now looking to raise as much as $100 billion in the coming years to fund its goal of developing artificial general intelligence (AGI), an AI advanced enough to improve its own capabilities.\n\n-Source. The hype around these models is causing a major bubble. Make sure you don’t get caught up\n\nSound like a good time? Let’s get right into it.\n\nImage Source\nSaturation of Benchmarks\n\nOne of the most important cornerstones of the hype behind AI was the performance increases that LLMs hit with multiple benchmarks. Every week, we found that GPT/a new Language Model was able to match/beat SOTA performance on a new benchmark or task. Who can forget the hype that we hit when we learned that GPT can pass the Bar Exam and even act as a doctor?\n\nThis led to a lot of speculation on AGI and how these models were developing so-called emergent abilities. However, peering behind the hood tells you a very different story.\n\nIn earlier years, people were improving significantly on the past year’s state of the art or best performance. This year across the majority of the benchmarks, we saw minimal progress to the point we decided not to include some in the report. For example, the best image classification system on ImageNet in 2021 had an accuracy rate of 91%; 2022 saw only a 0.1 percentage point improvement.\n\n-Source\n\nPeople involved in deep learning for a while will know an uncomfortable truth- AI performance has been increasingly saturated for a few years, way before we had investors and social media influencers blindly pushing the hype behind large language models. Machine Learning Researchers have burned more and more computation for increasingly smaller gains (sometimes under a percentage point).\n\nAI continued to post state-of-the-art results, but year-over-year improvement on many benchmarks continues to be marginal. Moreover, the speed at which benchmark saturation is being reached is increasing.\n\n- Stanford AI Index Report 2023\n\nSeen from this perspective, you should be less excited about these so-called amazing architectures. When it comes to performance- sure they can hit benchmarks- but at what cost? Deploying these models at any kind of scale would have you running out of computing budgets quicker than Haaland breaking goal-scoring records. Don’t forget, the scale that makes these models powerful also makes them extremely expensive to deploy in contexts where you have to make a lot of inferences ( Amazon Web Services estimates that “In deep learning applications, inference accounts for up to 90% of total operational costs”.).\n\nAccording to researchers who wrote, Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model[3], it took roughly 50.5 tons of CO2 equivalent to train the large-language model BLOOM. GPT-3 released over 500 tons of CO2 equivalent.\n\n- Once you’re done with article, check out my article on the ethics of Copilot\n\nHowever, that is far from the only issue that made GenAI far less promising than the internet would have led you to believe. Let’s now cover something that would come as a surprise to a lot of people who have graduated from ChatGPT University.\n\nPerformance\n\nHere’s something that would surprise you if you only read about GPT from online influencers telling you how to get ahead of 99% of people- these models are just not very good. When it comes to practically implementing Large Language Models into systems that are useful, efficient, and safe- these structures fall apart.\n\nWhen it comes to business use cases, these models are often lose to very simple models. The authors of “A Comparison of SVM against Pre-trained Language Models (PLMs) for Text Classification Tasks” compared the performance of LLMs with a puny SVM for text classification in various specialized business contexts. They fine-tuned and used the following models-\n\nAs I’ve covered here, these are the most popular models used in ML Engineering.\n\nThese models were stacked against an SVM and some old-fashioned feature engineering. The results are in the following table-\n\nAs you can see, SVMs match the performance of these large models. This is a huge win for them, given the much higher costs associated with the bigger models. Keep in mind, text classification is one of the core functions of these bigger models.\n\nThis extends beyond just text classification. Fast AI has an exceptional write-up investigating Github Copilot. One of their stand-out insights was- “According to OpenAI’s paper, Codex only gives the correct answer 29% of the time. And, as we’ve seen, the code it writes is generally poorly refactored and fails to take full advantage of existing solutions (even when they’re in Python’s standard library).” Just to drill home how overrated these models can be for coding, here are some of the problems with these models that the amazing Luca Rossi explored in his insightful AI & The Future of Coding 🤖. In it, he attempted to create the following with in Node-\n\nWrite a Telegram bot that answers my questions like ChatGPT, using OpenAI API\n\n-a relatively simple task, for any competen t developer\n\nThe code generated in Node didn’t work. Below is Luca’s analysis of the situation- It turns out, the AI used methods that do not exist in the Node library, probably inspired by the Python ones.\n\nBelow is Luca’s experience with Debugging-\n\nI replaced them and things worked fine. Two considerations:\n\nDebugging the code required me to study the openai and telegraf libraries, undoing 90% of the benefit of using the AI in the first place.\nDebugging was surprisingly hard, because these were not the kind of mistakes a normal person makes. When we debug, our brain is wired for looking at things we are more likely to get wrong. When you debug AI code, instead, literally anything can be wrong (maybe over time we will figure out the most common AI mistakes), which makes the work harder. In this case, AI completely made a method up — which is not something people usually do.\n\nAlong with this, Luca has some great insights into how using AI Coders would cause a lot of duplication of work in bug fixing and system design, would destroy innovation, use outdated methods, and a few other concerns. Would highly recommend checking out his work here-\n\nAI & The Future of Coding 🤖\nHey! Let's get something out of the way: this article has not been written by an AI. I know, it sucks. I am sorry…\n\nrefactoring.fm\n\nLet’s beyond ChatGPT and onto the allegedly early signs of AGI that were discovered in Microsoft’s 155 Page report- Sparks of Artificial General Intelligence: Early experiments with GPT-4. In it, we GPT-4 failing at some relatively simple tasks when it comes to a very simple task-\n\nWe see GPT-4 adding new information to the note, even though the prompt explicitly states- using exclusively the information above (and keep in mind this is Microsoft’s hype piece on GPT-4, so we don’t see the real disasters). Show this to the people that want to use GPT for doctors. If these systems are implemented recklessly, people will suffer. Regular readers know that I prefer not making sensational statements, but this is not me click-baiting you about AI’s existential threat. This is me telling you about a very real issue with these systems. If you’d like to read more about my investigation into GPT-4 and understand how the writers of the GPT-4 report ignored very real problems with their claims of AGI- read Observations on Microsoft’s Experiments with GPT-4.\n\nData-Centric AI\n\nPrior to this ‘bombshell’ admission and GenAI capturing the attention of everyone, there was another trendy buzzword that was moving through the Data Field- ‘Data-Centric AI’. The premise was relatively- we had spent a lot of time building better models and not enough time on improving our data transformation processes. Take a look at this quote from Andrew Ng at an event by MIT-\n\nAI systems need both code and data, and “all that progress in algorithms means it’s actually time to spend more time on the data,” Ng said at the recent EmTech Digital conference hosted by MIT Technology Review.\n\nOnce you’re done being wowed by the big names in the statement, tell me what part of that is truly surprising. It’s been well-known that data processing and curation is the most important part of the pipeline since the beginning. The reason that we saw this giant emphasis on models in research was two-fold-\n\nWe have big benchmarks/datasets (ImageNet for eg) for many of the standard tasks. In this case, the problem was in the architectures themselves. By standardizing datasets, we could test the effectiveness of various changes to the training pipelines.\nStreetlight Effect- The streetlight effect, or the drunkard’s search principle, is a type of observational bias that occurs when people only search for something where it is easiest to look. Combining this with the confirmation bias we see the mess of mindless scaling model size without attention given to other factors- people simply funding/copying whatever is already being done (in our case tweaking architectures). If all you see is working focused on changing architectures, then people will likely create work along a similar vein.\n\nBut the unsustainable nature of simply scaling up architectures has been well known to anyone who even remotely understands the field. Once you step outside the bubble of ML Academia and Big Tech AI- you will see that organizations lack the expertise/budgets/inclination to invest into implementing large models because the ROIs are not worth it.\n\nWe additionally do not utilize GPUs for inference in production. At our scale, outfitting each machine with one or more top-class GPUs would be prohibitively expensive, … that our models are relatively small compared to state-of-the-art models in other areas of deep learning (such as computer vision or natural language processing), we consider our approach much more economical.\n\n-This is from a writeup by the engineers at Zemanta, a leading advertising firm. Even at their level, SOTA DL isn’t really worth it. To learn more read- How to handle 300 million predictions per second over here\n\nThis is why we have seen the move away from bigger models and obscenely big data sets and instead focus on more intelligent design. Intelligent design was explicitly mentioned as one of the reasons Open Source was beating big companies like Google and Open in the infamous ‘Google has no Moat’ document-\n\nOpen-Source companies are doing things in weeks with $100 and 13B params that we struggle with at $10M and 540B.\n\n-Read my analysis of the situation here\n\nClosing\n\nCombining this together, here is a question I want to ask you- when exactly did we have an era of scaling up? At what point was it ever a good idea to implement more and more scaling- as opposed to focusing on better data selection, using ensembles/mixture of experts to reduce errors, or constraining the system to handle certain kinds of problems to avoid errors? People have been calling this unsustainable for a long time, way before general-purpose LLMs were a mainstay in ML.\n\nRather than an insightful statement about the future of AI, Sam Altman’s statement is a face-saving maneuver. As is becoming clear- the market is becoming increasingly saturated and OpenAI has no clear direction for making money. Even if they did corner the market, they can’t raise prices because customers can always switch to open-source models (and this assumes that the OpenAI models are meaningfully better- which is debatable). The bubble is bursting, the chickens have come home to roost, and this statement is Sam Altman is now scrambling to maintain the facade that these problems are under control.\n\nThat is it for this piece. I appreciate your time. As always, if you’re interested in reaching out to me or checking out my other work, links will be at the end of this email/post. If you like my writing, I would really appreciate an anonymous testimonial. You can drop it here. And if you found value in this write-up, I would appreciate you sharing it with more people. It is word-of-mouth referrals like yours that help me grow.\n"
          }
        },
        "ea3a996bd5964e638eb55bb23b3161dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "75%"
          }
        },
        "b339040d015a4eaf8d9abd09d092ab27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de7d220717b54acf92341ca78435f466": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b760a15e2bca4d78aaf5148ea6c695ce",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m∙∙∙\u001b[0m Running Summarization chain...\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">∙∙∙</span> Running Summarization chain...\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "b760a15e2bca4d78aaf5148ea6c695ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiomatricardi/LaMiniPower/blob/main/Study_of_Summarization_with_FlanT5LaMini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://mbzuai-nlp.github.io/LaMini-LM/images/LaMini-LM-solo.png\" height=300>\n",
        "\n",
        "\n",
        "# LaMini power: when a small guy can beat the Giants"
      ],
      "metadata": {
        "id": "xiYfGI3faWra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize YouTube subs  and text with_FlanT5LaMini\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Using Page Plain Text Chrome addon to download plain text from the Medium article \n",
        "Load it on Google Colab\n",
        "Use FlanT5-LaMini for Summarization and chat?"
      ],
      "metadata": {
        "id": "Z4e_7ABOMoB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOC"
      ],
      "metadata": {
        "id": "m1_Txof1VVTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Summarize YouTube subs  and text with_FlanT5LaMini](#scrollTo=Z4e_7ABOMoB3)\n",
        "\n",
        ">>[TOC](#scrollTo=m1_Txof1VVTl)\n",
        "\n",
        ">>[Install dependencies](#scrollTo=BAMCzEQ1VVkZ)\n",
        "\n",
        ">>[Prepare Models](#scrollTo=CgdXnNJBJ9Hd)\n",
        "\n",
        ">>>[Get HUGGINGFACEHUB_API_KEY](#scrollTo=3LKFoMvsKL0b)\n",
        "\n",
        ">>>[click on this link for only TEXT summarization](#scrollTo=stxfhSbXGalD)\n",
        "\n",
        ">>>[Youtube Summarization](#scrollTo=IKwe-n9KHU1g)\n",
        "\n",
        ">>>[Instructions](#scrollTo=HnZ9NS0IKh1O)\n",
        "\n",
        ">>[method for documents](#scrollTo=7CXaaYR6JhNq)\n",
        "\n",
        ">>[Decoder Only Models](#scrollTo=XPkZd9PH_uVI)\n",
        "\n",
        ">[Summarize from Copy Paste](#scrollTo=GXArZj0J_dAS)\n",
        "\n",
        ">>>[Add a spinner duiring summary generation](#scrollTo=IqtGAubMpPkl)\n",
        "\n",
        ">>[The Winner is... 🤗 MBZUAI/LaMini-Flan-T5-248M](#scrollTo=9_xFwoQKAuKN)\n",
        "\n",
        ">[Test with locally downloaded model](#scrollTo=AhO7RfFStfLP)\n",
        "\n",
        ">>>[Download Model from HuggingFace MBZUAI/LaMini-Flan-T5-248M and move it to Model folder](#scrollTo=MMToFuhdMdkL)\n",
        "\n",
        ">>[Import libraries](#scrollTo=w4PdoHUYBw52)\n",
        "\n",
        ">>[Iterate on the chunks, summarize one by one, and then summarize the summary](#scrollTo=0-qJvLLBU_iX)\n",
        "\n",
        ">>[Using LangChain CHAIN](#scrollTo=K45BGKhBfniS)\n",
        "\n",
        ">>>[The 🥇 text2text generation pipeline with   🦜🔗 LangChain win both in terms of time ⏲ and in terms of 👍 quality](#scrollTo=KBI463bjwFrT)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "h-1QwPFiVkYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "BAMCzEQ1VVkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install pytube and apply patch for captions and cypher"
      ],
      "metadata": {
        "id": "Sqtyfb6brm2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytube==12.1.3\n",
        "!git clone https://github.com/fabiomatricardi/pytubeFix.git\n",
        "%shell cp '/content/pytubeFix/1213-python10/captions.py' '/usr/local/lib/python3.10/dist-packages/pytube/captions.py'\n",
        "%shell cp '/content/pytubeFix/1213-python10/cipher.py' '/usr/local/lib/python3.10/dist-packages/pytube/cipher.py'"
      ],
      "metadata": {
        "id": "ThNREP33rsfr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install huggingface_hub\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install unstructured\n",
        "!pip install chromadb\n",
        "!pip install Cython\n",
        "!pip install tiktoken\n",
        "!pip install unstructured[local-inference]\n",
        "!pip install rich"
      ],
      "metadata": {
        "id": "Bg7dUzpCJ9qe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart Runtime {display-mode: \"form\"}\n",
        "import ipywidgets as widgets\n",
        "def restart(b):\n",
        "  exit()\n",
        "\n",
        "button2 = widgets.Button(\n",
        "    description='Restart Runtime',\n",
        "    disabled=False,\n",
        "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "button2.on_click(restart)\n",
        "button2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "036ad38d87bf475fa5ddcb4af224f1bf",
            "8d1057982ca741e98e335bd4e22ef12b",
            "a78d8fc0f41a44f383e652acc187b24b"
          ]
        },
        "id": "4-7s8rgY_Y89",
        "outputId": "2cf793f5-cbd6-4170-e630-539946ebae8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Restart Runtime', icon='check', style=ButtonStyle(), tooltip='Clic…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "036ad38d87bf475fa5ddcb4af224f1bf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Models"
      ],
      "metadata": {
        "id": "CgdXnNJBJ9Hd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get HUGGINGFACEHUB_API_KEY"
      ],
      "metadata": {
        "id": "3LKFoMvsKL0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "# PUT HERE YOUR HUGGING FACE API TOKEN shuold start with hf_XXXXXXX...\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_XXXXXXXX\"\n",
        "\n",
        "from langchain.document_loaders import TextLoader  #for textfiles\n",
        "from langchain.text_splitter import CharacterTextSplitter #text splitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings #for using HugginFace models\n",
        "from langchain.vectorstores import FAISS  #facebook vectorizationfrom langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "5avVrU_oKL0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want TEXT summarization, not from youtube<br>\n",
        "### [click on this link for only TEXT summarization](#scrollTo=AhO7RfFStfLP&uniqifier=1) "
      ],
      "metadata": {
        "id": "stxfhSbXGalD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Youtube Summarization"
      ],
      "metadata": {
        "id": "IKwe-n9KHU1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################### VIDEO AND SCRIPT SECTION #############################\n",
        "# Extract Video Informations for future use in PDF or word processor\n",
        "# Prepare the text for Summarization (no fuss, plain text)\n",
        "###########################################################################\n",
        "import ssl\n",
        "from pytube import YouTube as YT\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "# SSL for proxied internet access\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# paste here the url of the youtubevideo you want\n",
        "url = \"https://youtu.be/Xxxuw4_iCzw\" # or https://youtu.be/5g1z4Sr-UHM \n",
        "# we instantiate a YouTube object already with our \n",
        "myvideo = YT(url, use_oauth=True, allow_oauth_cache=True)\n",
        "# required only for the first time to know what languages are aavailable\n",
        "print(myvideo.title)\n",
        "print(myvideo.captions) #print the options of languages available\n",
        "code = input(\"input the code you want: \")\n",
        "print(\"Scraping subtitles...\")\n",
        "sub = myvideo.captions[code]\n",
        "caption = sub.generate_srt_captions()\n",
        "#print(caption)\n",
        "\n",
        "# Club Video Title, details and Description, only for printed version\n",
        "# not for the Sumarization one\n",
        "# possible in future to prepare for Markdown to PDF export\n",
        "import datetime\n",
        "m1 = f\"TITLE: {myvideo.title}\"+'\\n'\n",
        "m2 = f\"thumbnail url: {myvideo.thumbnail_url}\"+'\\n'\n",
        "m4 = f\"video Duration: {str(datetime.timedelta(seconds=myvideo.length))}\"+'\\n'\n",
        "m5 = \"----------------------------------------\"+'\\n'\n",
        "#m6 = textwrap.fill(myvideo.description, 80)+'\\n'  #solution not good\n",
        "m6 = myvideo.description+'\\n'\n",
        "m7 = \"----------------------------------------\"+'\\n'\n",
        "m_intro = m1+m2+m4+m5+m6+m7\n",
        "\n",
        "# Function to clean up the srt text\n",
        "def clean_sub(sub_list):\n",
        "    lines = sub_list\n",
        "    text = ''\n",
        "    for line in lines:\n",
        "        if re.search('^[0-9]+$', line) is None and re.search('^[0-9]{2}:[0-9]{2}:[0-9]{2}', line) is None and re.search('^$', line) is None:\n",
        "            text += ' ' + line.rstrip('\\n')\n",
        "        text = text.lstrip()\n",
        "    #print(text)\n",
        "    return text\n",
        "\n",
        "print(\"Transform subtitles to TEXT...\")\n",
        "srt_list = str(caption).split('\\n')  #generate a list with all lines\n",
        "final_text = clean_sub(srt_list)\n",
        "#print(final_text)\n",
        "#PREPARE A LONG STRING FOR THE SUMMARIZATION, no Video Description Here\n",
        "intro_summarization = 'Video Title: '+myvideo.title+' - (video url: )'+url+' ---  '+'\\n'\n",
        "#PREPARE A LONG TEXT with details and Video Description Here: to be used\n",
        "#as a note or Blog\n",
        "intro_blog = 'Video Title: '+myvideo.title+'\\n'+'Video url: '+url+'\\n'+'-------------'+'\\n'\n",
        "summarization_text = intro_summarization + final_text\n",
        "import textwrap\n",
        "blog_text = m_intro + textwrap.fill(final_text, 70)\n",
        "\n",
        "def correct_filename_cr(title):\n",
        "  string = title\n",
        "  finalfilename = ''.join(e for e in string if e.isalnum())+'_cr.txt'\n",
        "  return finalfilename\n",
        "def correct_filename_nocr(title):\n",
        "  string = title\n",
        "  finalfilename = ''.join(e for e in string if e.isalnum())+'_nocr.txt'\n",
        "  return finalfilename\n",
        "\n",
        "print(\"Saving to TXT file...\")\n",
        "filename_file = correct_filename_cr(myvideo.title) #with carriage return\n",
        "filename_summary = correct_filename_nocr(myvideo.title) #with no carriege return\n",
        "\n",
        "#prepare the text for the Blog or Notes\n",
        "#with carriage return. We want cool text only\n",
        "#for the report.\n",
        "import textwrap\n",
        "wrapped_text = textwrap.fill(final_text, 100)\n",
        "\n",
        "# write the cleaned up text into 2 file2\n",
        "# One with carriage returns, one withouth them\n",
        "with open(filename_file, 'w') as f:\n",
        "    f.write(blog_text)\n",
        "f.close()\n",
        "with open(filename_summary, 'w') as f:\n",
        "    f.write(summarization_text)\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBj7CWWxrnlN",
        "outputId": "4229da73-c72d-4759-b12a-bde9026cb32e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please open https://www.google.com/device and input code HJBZ-KZWM\n",
            "Press enter when you have completed this step.\n",
            "LangChain: Run Language Models Locally - Hugging Face Models\n",
            "{'a.en': <Caption lang=\"English (auto-generated)\" code=\"a.en\">}\n",
            "input the code you want: a.en\n",
            "Scraping subtitles...\n",
            "Transform subtitles to TEXT...\n",
            "Saving to TXT file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        "summarization_text is in the format for the Summarization Pipeline\n",
        "<br>\n",
        "blog_text is in the format for text processing or future studies\n",
        "> It may good to include also the summarization in the beginning of the text but it will more likely mess up with the vector indexing\n",
        " "
      ],
      "metadata": {
        "id": "HnZ9NS0IKh1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rich\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich import print\n",
        "\n",
        "########## INITIALIZE RICH CONSOLE  ##################\n",
        "console = Console()"
      ],
      "metadata": {
        "id": "4PyK0rzcKiOx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "# text splitter for create chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# FAISS  library for similaarity search\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "#print(wrap_text_preserve_newlines(str(documents[0])))   "
      ],
      "metadata": {
        "id": "ukNl5wGvKil1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents  #if youused above and want to see the result\n",
        "# added on 17 May 2023\n",
        "# Summarization is done with the CHAIN, not with the pipeline\n",
        "#-----------------------------------------------\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "llm9 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-Flan-T5-248M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "llm11 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-T5-223M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "\n",
        "def summarize_docs(textfile, llm):\n",
        "    \"\"\"\n",
        "    takes as argument the textfile name : str\n",
        "    and the llm (in this case from HuggingFaceHub)\n",
        "    To be explored how to do it locally for the pipeline...\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    loader = TextLoader(textfile)\n",
        "    docs = loader.load_and_split()\n",
        "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "    summary = chain.run(docs)\n",
        "    print(\"Summary for: \", textfile)\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n",
        "    summaries.append(summary)\n",
        "    \n",
        "    return summaries\n",
        "\n"
      ],
      "metadata": {
        "id": "EsC4Uc5KKil1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## method for documents"
      ],
      "metadata": {
        "id": "7CXaaYR6JhNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "# HERE 2 MODELS to be used with HuggingFace API for Summarization\n",
        "llm9 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-Flan-T5-248M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "llm11 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-T5-223M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "# Define Function that returns Summary of textfile\n",
        "def summarize_docs(textfile, llm):\n",
        "    \"\"\"\n",
        "    takes as argument the textfile name : str\n",
        "    and the llm (in this case from HuggingFaceHub)\n",
        "    To be explored how to do it locally for the pipeline...\n",
        "    \"\"\"\n",
        "    summaries = []\n",
        "    loader = TextLoader(textfile)\n",
        "    docs = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = 300,\n",
        "        chunk_overlap  = 20,\n",
        "    )\n",
        "    texts = text_splitter.create_documents(docs)\n",
        "    print(texts[0])\n",
        "    print(texts[1])\n",
        "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "    summary = chain.run(texts)\n",
        "    print(\"Summary for: \", textfile)\n",
        "    print(summary)\n",
        "    print(\"\\n\")\n",
        "    summaries.append(summary)\n",
        "    \n",
        "    return summaries\n",
        "\n"
      ],
      "metadata": {
        "id": "3cIT4cSmJk_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm6=HuggingFaceHub(repo_id=\"MBZUAI/LaMini-Flan-T5-783M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "llm2=HuggingFaceHub(repo_id=\"declare-lab/flan-alpaca-large\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "llm9 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-Flan-T5-248M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "llm11 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-T5-223M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "llm8 = HuggingFaceHub(repo_id=\"MBZUAI/LaMini-Flan-T5-77M\", model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "#fname = 'yourtextfile'\n",
        "fname = '/content/LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt'\n",
        "with open(fname) as f:\n",
        "    doc = f.read()\n",
        "summaries = []\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 900,\n",
        "    chunk_overlap  = 30,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.create_documents([doc])"
      ],
      "metadata": {
        "id": "14-_NDjDJgdm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain"
      ],
      "metadata": {
        "id": "Gbs4H5LGJmQl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open the text file for summarization, without carriage return\n",
        "# and split into chunks\n",
        "fname = '/content/LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt'\n",
        "with open(fname) as f:\n",
        "    doc = f.read()\n",
        "f.close() #always remember to close!\n",
        "summaries = []\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 400,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.create_documents([doc])\n",
        "\n",
        "# Call the Hugging Face API to run the Summarization Chain\n",
        "llm_id = 'MBZUAI/LaMini-T5-223M'\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "summaries.append(summary)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "# calculate the elapsed time and print to a Rich Console\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420,
          "referenced_widgets": [
            "66803ea0ce7f4ec68acce88a38225265",
            "760474961c4d4c1a8658a843c18b6a5b",
            "d594b17f6f064e41bdb06b708f03618a",
            "25654315c18b4b048e52930c863b37fc",
            "a743e8a8c58a4b08ab4d2f2b1c52a35c",
            "35ec5d3c9d5b4041b6457da8950f520f",
            "22160878f01641b5b6ba3b5905b1d4bd",
            "dbdbaeba33534a3a97bc42f63a53788c",
            "b8bce9d01d7e428a9ee0085db1b74318",
            "250363ed37f34983af743f12bbad1f41",
            "4c8a676e157048df82a10bc645aa09a0",
            "9c14699b57004bb4bef8a1ec6409dd9e",
            "379044263cde4328a8a89fcbede49c72",
            "bdea3e91c8bb4a8880cc3ae2a436e478",
            "ced0ced344a9419985db85b54917dd39",
            "b672dbc9e26049be86a67614cab1e1c5",
            "0bec3b920931444e95edf830512846a4",
            "562a98607363474ab2f45389a23203f7",
            "c6a3a6da1a624a289d19d745736f28c7",
            "f65ef08744a84b3ca05f2f48355ff1f5",
            "c5f2362c8f0b48f0bf1bcba83fd7f709",
            "978d968bb3c442bda3b6f36df85b4bca",
            "d4574a75d6d24f70886e080f4ceec26d",
            "169284926a014130a103a36d45512a02",
            "ecd4aaceec0c4de88c2ac2030e10a18f",
            "5551941125634c66a847c252d6b33598",
            "1ed7bbad325a47268a989f45e11715ce",
            "ac574835b2384a448653b9402463474b",
            "3f5f635fb96641f59e1a3ea5c2aaf679",
            "6b75c72d653e4e59a771d72420cd118a",
            "a2071a9c6066420f90cc78bfb5cb6a0d",
            "4eb9c994c277454ea892271f47c34f92",
            "ab7eeed594f74b6781794272d7bf0cc5",
            "6b8a7181194847828effdbbc51095e18",
            "1a69cf46858b41f5b4e7f8336b4a84b2",
            "1d18099087d6433baeb52552345984dc",
            "0f44ff721fe643079ef043f93f34e37e",
            "0c355bf574074dfaa8c395702e612415",
            "f4c3eedfd41242af8ad1e312b94f6369",
            "45f85312ac1d4bd6a05c3f94a0f9146f",
            "e4a7702dea714bd8a8d4482bd376e5aa",
            "6df82d349b9f46beb665e7bc7df5510e",
            "0b59dae32aae4cf3b9d167d0ef71eb49",
            "c6799d390e0d498c84be8143ddcae823"
          ]
        },
        "id": "5qR8RRayWd0V",
        "outputId": "12722dbe-c898-4bde-9a78-6778fa4e15de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing Summarization Chain\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing Summarization Chain</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66803ea0ce7f4ec68acce88a38225265"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c14699b57004bb4bef8a1ec6409dd9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4574a75d6d24f70886e080f4ceec26d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b8a7181194847828effdbbc51095e18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1453 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m[\u001b[0mBold green\u001b[1m]\u001b[0mSummary for:  \u001b[35m/content/\u001b[0m\u001b[95mLangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>Bold green<span style=\"font-weight: bold\">]</span>Summary for:  <span style=\"color: #800080; text-decoration-color: #800080\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-T5-223M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-T5-223M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The video shows how to use language models and hugging face models in apps, including using link chain,         │\n",
              "│ download models, and use hugging face pipelines. The model can be used locally using a lag chain, and the model │\n",
              "│ can be pre-trained for a specific type of model. The model can be used locally using a link chain, and the      │\n",
              "│ model can be used locally using a Google collab notebook. The video also explains how to use a text-to-text     │\n",
              "│ generation model and a decoder-only model using a local large. The model can be pre-trained for a specific      │\n",
              "│ model and can be used locally using a link chain. The video also provides a link to the video and a link to the │\n",
              "│ video.                                                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The video shows how to use language models and hugging face models in apps, including using link chain,         │\n",
              "│ download models, and use hugging face pipelines. The model can be used locally using a lag chain, and the model │\n",
              "│ can be pre-trained for a specific type of model. The model can be used locally using a link chain, and the      │\n",
              "│ model can be used locally using a Google collab notebook. The video also explains how to use a text-to-text     │\n",
              "│ generation model and a decoder-only model using a local large. The model can be pre-trained for a specific      │\n",
              "│ model and can be used locally using a link chain. The video also provides a link to the video and a link to the │\n",
              "│ video.                                                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:42\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m928109\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:42.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">928109</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the Hugging Face API to run the Summarization Chain\n",
        "# here we will use a bigger model, the LaMini-Flan-T5-783M\n",
        "llm_id = 'MBZUAI/LaMini-Flan-T5-783M'\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "YKwD75uzdJ-g",
        "outputId": "007f1e27-e481-40d8-f6de-738f744a03df"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing Summarization Chain\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing Summarization Chain</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1331 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary for: \u001b[0m \u001b[35m/content/\u001b[0m\u001b[95mLangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary for: </span> <span style=\"color: #800080; text-decoration-color: #800080\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-Flan-T5-783M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-Flan-T5-783M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article explains how to use Collage language models locally through API calls using the \"packing phase\" and │\n",
              "│ \"length chain\" platforms. It explains how to define a chain using a prompt template and how to use the pipeline │\n",
              "│ for local LN and prediction. The article also provides a crash course on the video and encourages comments. The │\n",
              "│ video is available for free on the Google Play Store.                                                           │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article explains how to use Collage language models locally through API calls using the \"packing phase\" and │\n",
              "│ \"length chain\" platforms. It explains how to define a chain using a prompt template and how to use the pipeline │\n",
              "│ for local LN and prediction. The article also provides a crash course on the video and encourages comments. The │\n",
              "│ video is available for free on the Google Play Store.                                                           │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:02:26\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m269167\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:02:26.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">269167</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "t8hoG9gxPiG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the smallest FlanT5 model\n",
        "llm_id = 'MBZUAI/LaMini-Flan-T5-77M'\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "kevhoIcIO7Oy",
        "outputId": "89ff6f7b-c852-438e-f7d6-a72c1281fc79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing Summarization Chain\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing Summarization Chain</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1497 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary for: \u001b[0m \u001b[35m/content/\u001b[0m\u001b[95mLangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary for: </span> <span style=\"color: #800080; text-decoration-color: #800080\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-Flan-T5-77M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-Flan-T5-77M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article explains how to use the LangChain: Run Language Models Locally and Hugging Face Models, which       │\n",
              "│ showcases the local language model and bigging face model. The package phase provides open source models that   │\n",
              "│ can be used in your own applications. The length chain is a powerful platform that allows you to build apps     │\n",
              "│ based on larger language models. The first option is to use collage language models from hugging face with link │\n",
              "│ chain the first. The second option is to download all these models locally and then use Hugging Face pipelines  │\n",
              "│ to interact with these models. The first option is to download all these models locally and then use Hugging    │\n",
              "│ Face pipelines to interact with these models. The second option is to download all these models locally and     │\n",
              "│ then use Hugging Face pipelines to interact with these models. The game phase API is expected to be able to run │\n",
              "│ the models locally using hugging face pipelines. The prompt template is a template that allows users to provide │\n",
              "│ specific variables to a model, and the prompt template is a template that allows users to pass on the prompt    │\n",
              "│ and what question they are expecting. The game phase API is expected to be able to run the models locally using │\n",
              "│ hugging face pipelines. The game phase API is expected to be able to run the models locally using hugging face  │\n",
              "│ pipelines. The prompt template is a template that allows users to pass on the prompt and what question they are │\n",
              "│ expecting. The prompt template is a template that allows users to pass on the prompt and what question they are │\n",
              "│ expecting. The model ID is used to identify the local uh L M using a pre-trained model for causal Ln. The model │\n",
              "│ ID is used to identify the local uh L M using the pipeline. The model ID is used to identify the local uh L M   │\n",
              "│ using the pipeline. The video provides instructions on how to describe a random chain using a hugging face      │\n",
              "│ pipeline from a lag chain using the prompt and local telem. The video provides a crash course on the algorithm  │\n",
              "│ to check the next video on like chain.                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article explains how to use the LangChain: Run Language Models Locally and Hugging Face Models, which       │\n",
              "│ showcases the local language model and bigging face model. The package phase provides open source models that   │\n",
              "│ can be used in your own applications. The length chain is a powerful platform that allows you to build apps     │\n",
              "│ based on larger language models. The first option is to use collage language models from hugging face with link │\n",
              "│ chain the first. The second option is to download all these models locally and then use Hugging Face pipelines  │\n",
              "│ to interact with these models. The first option is to download all these models locally and then use Hugging    │\n",
              "│ Face pipelines to interact with these models. The second option is to download all these models locally and     │\n",
              "│ then use Hugging Face pipelines to interact with these models. The game phase API is expected to be able to run │\n",
              "│ the models locally using hugging face pipelines. The prompt template is a template that allows users to provide │\n",
              "│ specific variables to a model, and the prompt template is a template that allows users to pass on the prompt    │\n",
              "│ and what question they are expecting. The game phase API is expected to be able to run the models locally using │\n",
              "│ hugging face pipelines. The game phase API is expected to be able to run the models locally using hugging face  │\n",
              "│ pipelines. The prompt template is a template that allows users to pass on the prompt and what question they are │\n",
              "│ expecting. The prompt template is a template that allows users to pass on the prompt and what question they are │\n",
              "│ expecting. The model ID is used to identify the local uh L M using a pre-trained model for causal Ln. The model │\n",
              "│ ID is used to identify the local uh L M using the pipeline. The model ID is used to identify the local uh L M   │\n",
              "│ using the pipeline. The video provides instructions on how to describe a random chain using a hugging face      │\n",
              "│ pipeline from a lag chain using the prompt and local telem. The video provides a crash course on the algorithm  │\n",
              "│ to check the next video on like chain.                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:50\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m393153\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:50.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">393153</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "llm_id = 'declare-lab/flan-alpaca-large'\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKcSY81fPExV",
        "outputId": "4f443742-2b88-420c-c9bb-25f33d57fc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1224 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary for:  /content/LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt\n",
            "with declare-lab/flan-alpaca-large \n",
            "\n",
            "-----------------------------------\n",
            "This video demonstrates how to use hugging face models to generate natural language models with minimal\n",
            "computational resources. It demonstrates how to use the API to access models, how to use hugging face\n",
            "pipelines to generate natural language models, and how to use a text generation model to generate text from a\n",
            "prompt.\n",
            "-----------------------------------\n",
            "All indexing completed in 0:00:21.820118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_id = 'MBZUAI/LaMini-T5-223M'\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "f6B7Zr2FUtOy",
        "outputId": "b53213b0-93c4-4e7f-b507-b549b7f91033"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing Summarization Chain\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing Summarization Chain</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1453 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary for: \u001b[0m \u001b[35m/content/\u001b[0m\u001b[95mLangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary for: </span> <span style=\"color: #800080; text-decoration-color: #800080\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-T5-223M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-T5-223M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The video shows how to use language models and hugging face models in apps, including using link chain,         │\n",
              "│ download models, and use hugging face pipelines. The model can be used locally using a lag chain, and the model │\n",
              "│ can be pre-trained for a specific type of model. The model can be used locally using a link chain, and the      │\n",
              "│ model can be used locally using a Google collab notebook. The video also explains how to use a text-to-text     │\n",
              "│ generation model and a decoder-only model using a local large. The model can be pre-trained for a specific      │\n",
              "│ model and can be used locally using a link chain. The video also provides a link to the video and a link to the │\n",
              "│ video.                                                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The video shows how to use language models and hugging face models in apps, including using link chain,         │\n",
              "│ download models, and use hugging face pipelines. The model can be used locally using a lag chain, and the model │\n",
              "│ can be pre-trained for a specific type of model. The model can be used locally using a link chain, and the      │\n",
              "│ model can be used locally using a Google collab notebook. The video also explains how to use a text-to-text     │\n",
              "│ generation model and a decoder-only model using a local large. The model can be pre-trained for a specific      │\n",
              "│ model and can be used locally using a link chain. The video also provides a link to the video and a link to the │\n",
              "│ video.                                                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:01\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m630381\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:01.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">630381</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the medium FlanT5 model\n",
        "llm_id = 'MBZUAI/LaMini-Flan-T5-248M'\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "kk9CLdnp6PZP",
        "outputId": "71ca4db7-280a-4db5-ed47-ae67827db7e1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing Summarization Chain\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing Summarization Chain</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1401 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary for: \u001b[0m \u001b[35m/content/\u001b[0m\u001b[95mLangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary for: </span> <span style=\"color: #800080; text-decoration-color: #800080\">/content/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LangChainRunLanguageModelsLocallyHuggingFaceModels_nocr.txt</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-Flan-T5-248M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-Flan-T5-248M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The video explains how to run language models locally using Hugging Face with link chain, which allows for      │\n",
              "│ access to local and locally-run models through API calls and Transformers. The article also provides            │\n",
              "│ instructions on how to fine-tune local or locally-run GPU models in a lag chain based on the Google collab      │\n",
              "│ notebook. The team is analyzing the quality of the model used through the game phase API and identifying ways   │\n",
              "│ to run locally using hugging face pipelines. The video provides a crash course on Lang chain and recommends     │\n",
              "│ watching the next video on Like Chain.                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The video explains how to run language models locally using Hugging Face with link chain, which allows for      │\n",
              "│ access to local and locally-run models through API calls and Transformers. The article also provides            │\n",
              "│ instructions on how to fine-tune local or locally-run GPU models in a lag chain based on the Google collab      │\n",
              "│ notebook. The team is analyzing the quality of the model used through the game phase API and identifying ways   │\n",
              "│ to run locally using hugging face pipelines. The video provides a crash course on Lang chain and recommends     │\n",
              "│ watching the next video on Like Chain.                                                                          │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:44\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m993300\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:44.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">993300</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Only Models"
      ],
      "metadata": {
        "id": "XPkZd9PH_uVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_id = 'MBZUAI/LaMini-GPT-1.5B' #'MBZUAI/LaMini-Neo-125M' API non funziona\n",
        "import datetime\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarization Chain\")\n",
        "\n",
        "# Call the Chain and run it\n",
        "# Decoder Only models cannot have more than 500 as max lenght\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0.2, \"max_length\":500})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "\n",
        "# print the results with rich text format\n",
        "console.print(\"[bold green]Summary for: \", fname)\n",
        "console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FghH293MVr2k",
        "outputId": "bd950b76-c962-417f-f4a9-9088124fd4a7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing Summarization Chain\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing Summarization Chain</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4a30353b5b07>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHuggingFaceHub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_summarize_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map_reduce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# print the results with rich text format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_only_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             outputs = (\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/base.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Other keys are assumed to be needed for LLM prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mother_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_key\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         output, extra_return_dict = self.combine_docs(\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mother_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/combine_documents/map_reduce.py\u001b[0m in \u001b[0;36mcombine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mThis\u001b[0m \u001b[0mreducing\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdone\u001b[0m \u001b[0mrecursively\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mneeded\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmany\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m         results = self.llm_chain.apply(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;31m# FYI - this is parallelized and so it is fast.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_variable_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    152\u001b[0m         )\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_prompts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    133\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 output = (\n\u001b[0;32m--> 185\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             text = (\n\u001b[0;32m--> 411\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/llms/huggingface_hub.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error raised by inference API: {response['error']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;31m# Text generation return includes the starter text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error raised by inference API: Service Unavailable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Raised API error, but I am not sure if this is because of the Decoder-only model`"
      ],
      "metadata": {
        "id": "wsPrJdJF_J8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_id = 'MBZUAI/LaMini-Cerebras-111M'  #'MBZUAI/LaMini-GPT-124M' not working API\n",
        "summaries = []\n",
        "import datetime\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "#####################################################################################\n",
        "#               for TEXT-GENERATION models max_lenght CANNOT be > than 500          #\n",
        "#####################################################################################\n",
        "llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0.5, \"max_length\":480})\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "summary = chain.run(texts)\n",
        "print(\"Summary for: \", fname)\n",
        "print(f\"with {llm_id} \\n\")\n",
        "print(\"-----------------------------------\")\n",
        "summaries.append(summary)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start  \n",
        "print(wrap_text_preserve_newlines(str(summaries[0])))\n",
        "print(\"-----------------------------------\")\n",
        "print(f\"All indexing completed in {delta}\")"
      ],
      "metadata": {
        "id": "YNxso4gWXYev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Raised API error, but I am not sure if this is because of the Decoder-only model`"
      ],
      "metadata": {
        "id": "zM28lKi8O1el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "c51yFL-C_btF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarize from Copy Paste"
      ],
      "metadata": {
        "id": "GXArZj0J_dAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Image, display\n",
        "from tqdm import tqdm\n",
        "\n",
        "global testo \n",
        "testo = widgets.Textarea(\n",
        "                        value='',\n",
        "                        placeholder=f'Paste here the text you want to summarize',\n",
        "                        description=f'TEXT:',\n",
        "                        disabled=False,\n",
        "                        rows = 20,   \n",
        "                        layout = widgets.Layout( width='75%')\n",
        "                            )\n",
        "# Club text spitting and Summarization Chain into a Function\n",
        "# on the widgets a (b) parameter is mandatory, even if you\n",
        "# don't pass any variable.\n",
        "#-----------------------------------------------------------\n",
        "def btn_summarization(b):\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "          # Set a really small chunk size, just to show.\n",
        "          chunk_size = 800,\n",
        "          chunk_overlap  = 30,\n",
        "          length_function = len,\n",
        "          )\n",
        "      texts = text_splitter.create_documents([testo.value])\n",
        "      llm_id = 'MBZUAI/LaMini-Flan-T5-248M'\n",
        "      fname = \"From Copy/Paste\"\n",
        "      import datetime\n",
        "      summaries = []\n",
        "      start = datetime.datetime.now() #not used now but useful\n",
        "      llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "      chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "      summary = chain.run(texts)\n",
        "      # print the results with rich text format\n",
        "      console.print(\"[bold green]Summary for: \", fname)\n",
        "      console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "      summaries.append(summary)\n",
        "      stop = datetime.datetime.now() #not used now but useful  \n",
        "      delta = stop-start  \n",
        "      print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "      console.print(f\"[red bold]Summarization completed in {delta}\")\n",
        "\n",
        "#--------Define a pushButton and associate Click to the Function-------\n",
        "btn_sum = widgets.Button(\n",
        "    description='Start Summarization',\n",
        "    disabled=False,\n",
        "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "btn_sum.on_click(btn_summarization)\n",
        "#--------------------Display the 2 elements------------------------\n",
        "display(testo)\n",
        "display(btn_sum)\n"
      ],
      "metadata": {
        "id": "TlDINjlV_hP1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673,
          "referenced_widgets": [
            "876a0578339847918b25f9400b5c5846",
            "1eaa176a2f7346908488860c929df152",
            "9537cb85b7a24257b8dbe4a222502c7c",
            "68b523f0309a47a889f109066eaf22a7",
            "c2ce562277d440ce932aecc3e2fde479",
            "66231ddcd15142e384edf76322ae9c86"
          ]
        },
        "outputId": "60937cab-ee73-4408-ccd1-3497af33d452"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='TEXT:', layout=Layout(width='75%'), placeholder='Paste here the text you want …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876a0578339847918b25f9400b5c5846"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='info', description='Start Summarization', icon='check', style=ButtonStyle(), tooltip='Cli…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68b523f0309a47a889f109066eaf22a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1249 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary for: \u001b[0m From Copy/Paste\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary for: </span> From Copy/Paste\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-Flan-T5-248M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-Flan-T5-248M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses how to use LLMs on domain-specific knowledge base using Retrieval-Augmented Generation,   │\n",
              "│ which involves presenting LLMs with relevant information and allowing them to ask questions about the internal  │\n",
              "│ knowledge base. The article also discusses how to fine-tune a bookworm using a fine-tuned LLM and how it can be │\n",
              "│ used to unlock information stored in a specific knowledge base. The article also discusses the importance of    │\n",
              "│ fine-tuning LLMs and how it works best if the specific knowledge base does not contain super specific jargon.   │\n",
              "│ The article also discusses the challenges of setting up a successful RAG solution, including integrating APIs   │\n",
              "│ and generative capabilities, increasing traction, and addressing the potential for mediocre versions. The       │\n",
              "│ article also suggests looking for a RAG-based solution that offers specific knowledge and business value, and   │\n",
              "│ suggests looking for a demo solution to help government staff answer parliamentary questions more easily using  │\n",
              "│ technology.                                                                                                     │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses how to use LLMs on domain-specific knowledge base using Retrieval-Augmented Generation,   │\n",
              "│ which involves presenting LLMs with relevant information and allowing them to ask questions about the internal  │\n",
              "│ knowledge base. The article also discusses how to fine-tune a bookworm using a fine-tuned LLM and how it can be │\n",
              "│ used to unlock information stored in a specific knowledge base. The article also discusses the importance of    │\n",
              "│ fine-tuning LLMs and how it works best if the specific knowledge base does not contain super specific jargon.   │\n",
              "│ The article also discusses the challenges of setting up a successful RAG solution, including integrating APIs   │\n",
              "│ and generative capabilities, increasing traction, and addressing the potential for mediocre versions. The       │\n",
              "│ article also suggests looking for a RAG-based solution that offers specific knowledge and business value, and   │\n",
              "│ suggests looking for a demo solution to help government staff answer parliamentary questions more easily using  │\n",
              "│ technology.                                                                                                     │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:59\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m921515\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:59.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">921515</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a spinner duiring summary generation"
      ],
      "metadata": {
        "id": "IqtGAubMpPkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Image, display\n",
        "from tqdm import tqdm\n",
        "\n",
        "global testo \n",
        "testo = widgets.Textarea(\n",
        "                        value='',\n",
        "                        placeholder=f'Paste here the text you want to summarize',\n",
        "                        description=f'TEXT:',\n",
        "                        disabled=False,\n",
        "                        rows = 20,   \n",
        "                        layout = widgets.Layout( width='75%')\n",
        "                            )\n",
        "# Club text spitting and Summarization Chain into a Function\n",
        "# on the widgets a (b) parameter is mandatory, even if you\n",
        "# don't pass any variable.\n",
        "# Add a Spinner to monitor the progress ongoing\n",
        "#-----------------------------------------------------------\n",
        "def btn_summarization(b):\n",
        "      with console.status('Running Summarization pipeline...', spinner='material'):\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            # Set a really small chunk size, just to show.\n",
        "            chunk_size = 700,\n",
        "            chunk_overlap  = 30,\n",
        "            length_function = len,\n",
        "            )\n",
        "        texts = text_splitter.create_documents([testo.value])\n",
        "        llm_id = 'MBZUAI/LaMini-Flan-T5-248M'\n",
        "        fname = \"From Copy/Paste\"\n",
        "        import datetime\n",
        "        summaries = []\n",
        "        start = datetime.datetime.now() #not used now but useful\n",
        "        llm=HuggingFaceHub(repo_id=llm_id, model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "        chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "        summary = chain.run(texts)\n",
        "        # print the results with rich text format\n",
        "        console.print(\"[bold green]Summary for: \", fname)\n",
        "        console.print(f\"[italic black] with {llm_id} \\n\")\n",
        "        summaries.append(summary)\n",
        "        stop = datetime.datetime.now() #not used now but useful  \n",
        "        delta = stop-start  \n",
        "        print(Panel(str(summaries[0]), title='AI Summarization'))\n",
        "        console.print(f\"[red bold]Summarization completed in {delta}\")\n",
        "\n",
        "        btn_dwn = widgets.Button(\n",
        "                                  description='Download Summary',\n",
        "                                  disabled=False,\n",
        "                                  button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
        "                                  tooltip='Click me',\n",
        "                                  icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        "                              )\n",
        "        def download_sum(b):\n",
        "          with open('generated_summary.txt', 'w') as f:\n",
        "            f.write(str(summaries[0]))\n",
        "          f.close()\n",
        "          console.print(\"[bold red blink]File Downloaded into Drive\")\n",
        "          \n",
        "        btn_dwn.on_click(download_sum)\n",
        "        display(btn_dwn)\n",
        "        \n",
        "\n",
        "#--------Define a pushButton and associate Click to the Function-------\n",
        "btn_sum = widgets.Button(\n",
        "    description='Start Summarization',\n",
        "    disabled=False,\n",
        "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "btn_sum.on_click(btn_summarization)\n",
        "#--------------------Display the 2 elements------------------------\n",
        "display(testo)\n",
        "display(btn_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723,
          "referenced_widgets": [
            "3c135f52d2884d4a961eacb375ad0738",
            "55b2e5aae67c4d27add54f00722df8de",
            "cb55b792c7e348989f9efa4056a5b74a",
            "eb98435005ff44ea8027e7579c1415e9",
            "e78843edd3af4d7492423c84cb140e87",
            "18cf834295c34157aba19f3169e95f79",
            "c9ed1f36b73d4dd69e109f895d64d13d",
            "1c9f6a3a048e4e528e240d48107488c2",
            "6152573912dd4e3eb8f320de8271b981",
            "b1a7aa03f79b42eaae388f494c0adbc2",
            "e1264139a204409a923368c7e79c5640"
          ]
        },
        "id": "0FH2w3jLf6dD",
        "outputId": "633d0d0d-4f48-42a7-ac9c-797d39d107de"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='TEXT:', layout=Layout(width='75%'), placeholder='Paste here the text you want …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c135f52d2884d4a961eacb375ad0738"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='info', description='Start Summarization', icon='check', style=ButtonStyle(), tooltip='Cli…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb98435005ff44ea8027e7579c1415e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9ed1f36b73d4dd69e109f895d64d13d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Token indices sequence length is longer than the specified maximum sequence length for this model (1496 > 1024). \n",
              "Running this sequence through the model will result in indexing errors\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Token indices sequence length is longer than the specified maximum sequence length for this model (1496 &gt; 1024). \n",
              "Running this sequence through the model will result in indexing errors\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;32mSummary for: \u001b[0m From Copy/Paste\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Summary for: </span> From Copy/Paste\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[3;30m with MBZUAI/LaMini-Flan-T5-248M \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000000; text-decoration-color: #000000; font-style: italic\"> with MBZUAI/LaMini-Flan-T5-248M </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╭────────────────────────────── AI Summarization ──────────────────────────────╮\n",
            "│ The article discusses how to leverage LLMs on a domain-specific knowledge    │\n",
            "│ base, including using RAG to Riches to talk to data, Retrieval-Augmented     │\n",
            "│ Generation to unlock information stored in a specific knowledge base, and    │\n",
            "│ how to fine-tune LLMs to follow the tone and lingo present in the knowledge  │\n",
            "│ base. The article also discusses the challenges of setting up a successful   │\n",
            "│ RAG solution, including avoiding hallucination and ensuring that LLM         │\n",
            "│ solutions are kept in production. The article also suggests exploring an     │\n",
            "│ RAG-based solution if interested in the concept of Retrieval-Augmented       │\n",
            "│ Generation, and demonstrating the effectiveness of RAG in answering          │\n",
            "│ parliamentary questions.                                                     │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:01\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m102382\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:01.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">102382</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='danger', description='Download Summary', icon='check', style=ButtonStyle(), tooltip='Clic…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6152573912dd4e3eb8f320de8271b981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;5;31mFile Downloaded into Drive\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">File Downloaded into Drive</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Winner is... 🤗 MBZUAI/LaMini-Flan-T5-248M"
      ],
      "metadata": {
        "id": "9_xFwoQKAuKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test with locally downloaded model\n",
        "\n",
        "If you want to try YoutubeSummarization [click here](#scrollTo=IKwe-n9KHU1g&uniqifier=1)"
      ],
      "metadata": {
        "id": "AhO7RfFStfLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install mkl mkl-include\n",
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install git+https://github.com//huggingface/transformers \n",
        "!pip install accelerate\n",
        "!pip install rich"
      ],
      "metadata": {
        "id": "pZyIwQ2UBnZ5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Restart Runtime {display-mode: \"form\"}\n",
        "import ipywidgets as widgets\n",
        "def restart(b):\n",
        "  exit()\n",
        "\n",
        "button2 = widgets.Button(\n",
        "    description='Restart Runtime',\n",
        "    disabled=False,\n",
        "    button_style='warning', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check' # (FontAwesome names without the `fa-` prefix)\n",
        ")\n",
        "button2.on_click(restart)\n",
        "button2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8284ed90792d4cc5a5ee58cf9d7398d9",
            "98a2a1a949e0434eac16562fed3045f4",
            "8cdc90e710c84a8889698a1a5a482b79"
          ]
        },
        "outputId": "f6fdf0c0-a976-4a4a-d0d5-4fc38ff25234",
        "id": "r0r7ikjKHqMV"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='warning', description='Restart Runtime', icon='check', style=ButtonStyle(), tooltip='Clic…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8284ed90792d4cc5a5ee58cf9d7398d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Model from HuggingFace *MBZUAI/LaMini-Flan-T5-248M* and move it to Model folder"
      ],
      "metadata": {
        "id": "MMToFuhdMdkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/.gitattributes\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/.gitignore\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/README.md\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/config.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/generation_config.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/pytorch_model.bin\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/special_tokens_map.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/spiece.model\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/tokenizer.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/tokenizer_config.json\n",
        "!wget https://huggingface.co/MBZUAI/LaMini-Flan-T5-248M/resolve/main/training_args.bin\n",
        "!mkdir model\n",
        "!mv /content/.gitattributes /content/model/.gitattributes\n",
        "!mv /content/.gitignore /content/model/.gitignore\n",
        "!mv /content/README.md /content/model/README.md\n",
        "!mv /content/config.json  /content/model/config.json\n",
        "!mv /content/generation_config.json  /content/model/generation_config.json\n",
        "!mv /content/pytorch_model.bin  /content/model/pytorch_model.bin\n",
        "!mv /content/special_tokens_map.json  /content/model/special_tokens_map.json\n",
        "!mv /content/spiece.model  /content/model/spiece.model\n",
        "!mv /content/tokenizer.json  /content/model/tokenizer.json\n",
        "!mv /content/tokenizer_config.json  /content/model/tokenizer_config.json\n",
        "!mv /content/training_args.bin  /content/model/training_args.bin"
      ],
      "metadata": {
        "id": "JEYKg_j7K0bC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "w4PdoHUYBw52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lw92t99YEEcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## PROCEED WITH TEXT SPLITTER #########################\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 400,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.create_documents([testo.value])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkGeKaO_W4tl",
        "outputId": "3cf36071-a463-4978-aebe-fca3a4d3912a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.utilities.powerbi:Could not import azure.core python package.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## PROCEED WITH TEXT SPLITTER #########################\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.create_documents([testo.value])\n",
        "\n",
        "checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint,\n",
        "                                                    device_map='auto',\n",
        "                                                    torch_dtype=torch.float32)\n",
        "console.print(\"[yellow bold] Inizializing pipeline\")\n",
        "pipe_sum = pipeline('summarization', \n",
        "                    model = base_model,\n",
        "                    tokenizer = tokenizer,\n",
        "                    max_length = 400, \n",
        "                    min_length = 25\n",
        "                    )\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "# taking from ipyWidget textarea content\n",
        "with console.status('Running Summarization pipeline...', spinner='arrow3'):\n",
        "  result = pipe_sum(testo.value)\n",
        "  stop = datetime.datetime.now() #not used now but useful  \n",
        "  delta = stop-start  \n",
        "  print(Panel(result[0]['summary_text'], title='AI Summarization'))\n",
        "  console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "b1dcfb1c199d4f92a797d0fd3724ee1e",
            "e15cbded17404d609307ad99a67463a0"
          ]
        },
        "id": "_CrLupkeXeGE",
        "outputId": "461d03ad-c750-400c-8dcf-04217ee3982b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (3172 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1dcfb1c199d4f92a797d0fd3724ee1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses how the era of large language models is over and how the hype surrounding them is causing │\n",
              "│ a major bubble in the AI space. The article also discusses the importance of data processing and curation in    │\n",
              "│ implementing large models, as well as the need for companies to invest in improving their data transformation   │\n",
              "│ processes. The author asks for an anonymous testimonial to help them grow.                                      │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses how the era of large language models is over and how the hype surrounding them is causing │\n",
              "│ a major bubble in the AI space. The article also discusses the importance of data processing and curation in    │\n",
              "│ implementing large models, as well as the need for companies to invest in improving their data transformation   │\n",
              "│ processes. The author asks for an anonymous testimonial to help them grow.                                      │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:02:33\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m018731\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:02:33.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">018731</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With shorter version of the text it seems good"
      ],
      "metadata": {
        "id": "7CXMHhFFN-oD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This text is already with limited number of tokens...\n",
        "# and can run directly in the pipeline.\n",
        "# But if your text is longer you will get alarm from tokenizer\n",
        "#\n",
        "#Token indices sequence length is longer than the specified maximum sequence length for this model (3103 > 512). \n",
        "#Running this sequence through the model will result in indexing errors\n",
        "# 598 words, 3500 charachters\n",
        "#################################################################################################################\n",
        "copied_text = \"\"\"\n",
        "Title: Dear Sam Altman- There was never an era of making models bigger  Recently, the internet caught fire with a particular admission from Sam Altman- The Era of Large Language Models is over, further progress will not come from making models bigger.  “We’ll make them better in other ways.” In this article, I’m here to argue something that would be considered blasphemy to many people in the AI Space- the age of mindlessly scaling models has never been here. When we look at the data and actually compare the results- it has always been clear that throwing more and more data and increasing parameter size was always doomed to fail- long before we started hitting the scaling limits that GPT-4 is starting to hit. By understanding how we could have foreseen this problem, we can avoid making these mistakes in the future- saving everyone a lot of time, money, and attention.\n",
        "Seen from this perspective, you should be less excited about these so-called amazing architectures. When it comes to performance- sure they can hit benchmarks- but at what cost? Deploying these models at any kind of scale would have you running out of computing budgets quicker than Haaland breaking goal-scoring records. Don’t forget, the scale that makes these models powerful also makes them extremely expensive to deploy in contexts where you have to make a lot of inferences.\n",
        "Here’s something that would surprise you if you only read about GPT from online influencers telling you how to get ahead of 99% of people- these models are just not very good. When it comes to practically implementing Large Language Models into systems that are useful, efficient, and safe- these structures fall apart. When it comes to business use cases, these models are often lose to very simple models.\n",
        "Prior to this ‘bombshell’ admission and GenAI capturing the attention of everyone, there was another trendy buzzword that was moving through the Data Field- ‘Data-Centric AI’. The premise was relatively- we had spent a lot of time building better models and not enough time on improving our data transformation processes. Take a look at this quote from Andrew Ng at an event by MIT-\n",
        "AI systems need both code and data, and “all that progress in algorithms means it’s actually time to spend more time on the data,” Ng said at the recent EmTech Digital conference hosted by MIT Technology Review.\n",
        "It’s been well-known that data processing and curation is the most important part of the pipeline since the beginning.  This is why we have seen the move away from bigger models and obscenely big data sets and instead focus on more intelligent design. Intelligent design was explicitly mentioned as one of the reasons Open Source was beating big companies like Google.\n",
        "Combining this together, here is a question I want to ask you- when exactly did we have an era of scaling up? At what point was it ever a good idea to implement more and more scaling- as opposed to focusing on better data selection, using ensembles/mixture of experts to reduce errors, or constraining the system to handle certain kinds of problems to avoid errors? People have been calling this unsustainable for a long time, way before general-purpose LLMs were a mainstay in ML.\n",
        "Rather than an insightful statement about the future of AI, Sam Altman’s statement is a face-saving maneuver. The bubble is bursting, the chickens have come home to roost, and this statement is Sam Altman is now scrambling to maintain the facade that these problems are under control.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CpKTJZ_LONq0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint,\n",
        "                                                    device_map='auto',\n",
        "                                                    torch_dtype=torch.float32)\n",
        "console.print(\"[yellow bold] Inizializing pipeline\")\n",
        "pipe_sum = pipeline('summarization', \n",
        "                    model = base_model,\n",
        "                    tokenizer = tokenizer,\n",
        "                    max_length = 550, \n",
        "                    min_length = 25\n",
        "                    )\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "# taking from ipyWidget textarea content\n",
        "with console.status('Running Summarization pipeline...', spinner='arrow3'):\n",
        "  result = pipe_sum(copied_text)\n",
        "  stop = datetime.datetime.now() #not used now but useful  \n",
        "  delta = stop-start  \n",
        "  print(Panel(result[0]['summary_text'], title='AI Summarization'))\n",
        "  console.print(f\"[red bold]Summarization completed in {delta}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258,
          "referenced_widgets": [
            "00d58c99cce447f7a16e6fad51fabbf5",
            "b6a9b60406254d66885a0141dcb1478d"
          ]
        },
        "id": "sZUgeye_OIde",
        "outputId": "aa5f44b5-a2df-4edf-cc4a-92c3837662d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;33m Inizializing pipeline\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> Inizializing pipeline</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (774 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00d58c99cce447f7a16e6fad51fabbf5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ Sam Altman believes that the era of large language models is over and that further progress will not come from  │\n",
              "│ making models bigger. He argues that throwing more and more data and increasing parameter size was always       │\n",
              "│ doomed to fail and that the scale that makes these models powerful also makes them extremely expensive to       │\n",
              "│ deploy in contexts where you have to make a lot of inferences. The article also discusses the importance of     │\n",
              "│ data processing and curation in AI systems, and the move away from bigger models and obscenely big data sets to │\n",
              "│ focus on more intelligent design. The author questions when exactly it was a good idea to implement more        │\n",
              "│ scaling and focusing on better data selection, using ensembles/mixture of experts to reduce errors, or          │\n",
              "│ constraining the system to handle certain kinds of problems to avoid errors.                                    │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ Sam Altman believes that the era of large language models is over and that further progress will not come from  │\n",
              "│ making models bigger. He argues that throwing more and more data and increasing parameter size was always       │\n",
              "│ doomed to fail and that the scale that makes these models powerful also makes them extremely expensive to       │\n",
              "│ deploy in contexts where you have to make a lot of inferences. The article also discusses the importance of     │\n",
              "│ data processing and curation in AI systems, and the move away from bigger models and obscenely big data sets to │\n",
              "│ focus on more intelligent design. The author questions when exactly it was a good idea to implement more        │\n",
              "│ scaling and focusing on better data selection, using ensembles/mixture of experts to reduce errors, or          │\n",
              "│ constraining the system to handle certain kinds of problems to avoid errors.                                    │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:00:54\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m992720\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:00:54.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">992720</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Time to test the results if we club chucks summarized into a final<br>\n",
        "summarization"
      ],
      "metadata": {
        "id": "yKn1m9m_Oiku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kF1vRDmsOI2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iterate on the chunks, summarize one by one, and then summarize the summary"
      ],
      "metadata": {
        "id": "0-qJvLLBU_iX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that return the Summarized text\n",
        "def AI_chunk_Summary(checkpoint, text):\n",
        "    \"\"\"\n",
        "    checkpoint is in the format of relative path\n",
        "    example:  checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "    text it is either a long string or a input long string or a loaded document into string\n",
        "\n",
        "    \"\"\"\n",
        "    #checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "    checkpoint = checkpoint\n",
        "    tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
        "    base_model = T5ForConditionalGeneration.from_pretrained(checkpoint,\n",
        "                                                        device_map='auto',\n",
        "                                                        torch_dtype=torch.float32)\n",
        "    pipe_sum = pipeline('summarization', \n",
        "                        model = base_model,\n",
        "                        tokenizer = tokenizer,\n",
        "                        max_length = 250, \n",
        "                        min_length = 25\n",
        "                        )\n",
        "    result = pipe_sum(text)\n",
        "    sum_text = result[0]['summary_text']\n",
        "    return sum_text"
      ],
      "metadata": {
        "id": "xRD4yZPBVgJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 1200,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "#texts = text_splitter.create_documents([testo.value]) #valid to create Document Object\n",
        "texts = text_splitter.split_text(testo.value)\n",
        "# with testo.value is giving 33 chunks..."
      ],
      "metadata": {
        "id": "E5KRbCrUVGYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGcypLfUY_to",
        "outputId": "3b7b1a65-ac30-4b0a-a6c5-da90b5583c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_sum = AI_chunk_Summary(chkpoint, texts[0])\n",
        "console.print (small_sum)\n",
        "type(small_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "5-2IECSaXkTk",
        "outputId": "b71bf73e-b8e4-4c25-bf3e-37951067f0f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "The article discusses how to leverage LLMs on a domain-specific knowledge base by using Retrieval-Augmented \n",
              "Generation to talk to your data. The article also mentions OpenAI's GPT-\u001b[1;36m4\u001b[0m and Meta's LLamA as examples of LLM \n",
              "models that were trained on publicly available data.\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The article discusses how to leverage LLMs on a domain-specific knowledge base by using Retrieval-Augmented \n",
              "Generation to talk to your data. The article also mentions OpenAI's GPT-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span> and Meta's LLamA as examples of LLM \n",
              "models that were trained on publicly available data.\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = ''\n",
        "chkpoint = '/content/model/'\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "for i in range(0,len(texts)): \n",
        "  small_sum = AI_chunk_Summary(chkpoint, texts[i])\n",
        "  final_text = final_text + small_sum\n",
        "Summary = AI_chunk_Summary(chkpoint, final_text)\n",
        "stop = datetime.datetime.now() #not used now but useful  \n",
        "delta = stop-start\n",
        "print(Panel(Summary, title='AI Summarization'))  \n",
        "console.print(f\"[red bold]Summarization completed in {delta}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "RXfGDvZOWBKz",
        "outputId": "2861631e-50fd-4271-ccce-57cea88ee9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 250, but your input_length is only 234. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=117)\n",
            "Your max_length is set to 250, but your input_length is only 244. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=122)\n",
            "Your max_length is set to 250, but your input_length is only 220. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
            "Your max_length is set to 250, but your input_length is only 221. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=110)\n",
            "Your max_length is set to 250, but your input_length is only 133. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n",
            "Your max_length is set to 250, but your input_length is only 196. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=98)\n",
            "Your max_length is set to 250, but your input_length is only 245. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=122)\n",
            "Your max_length is set to 250, but your input_length is only 51. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1260 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses how to leverage LLMs on a domain-specific knowledge base by using Retrieval-Augmented     │\n",
              "│ Generation (RAG), which is a semantic search engine that uses vectorial representations of document sections to │\n",
              "│ determine the meaning stored in each section. Fine-tuned LLM can be used to handle specific jargon and          │\n",
              "│ incorporate it into the RAG architecture to reap the combined benefits. The main challenges with setting up a   │\n",
              "│ successful RAG solution include ensuring the quality of the generative AI, attracting and retaining a large     │\n",
              "│ number of users, managing the traction of the system, and balancing the benefits of RAG with the potential for  │\n",
              "│ market growth. Proper setup takes time and attention, and selecting the right LLM is important for cost and     │\n",
              "│ data management. LLMOps can support government staff in answering parliamentary questions more easily, with     │\n",
              "│ specific knowledge of a moderate database of knowledge articles that contain useful information that is not     │\n",
              "│ easily found on the world-wide web.                                                                             │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses how to leverage LLMs on a domain-specific knowledge base by using Retrieval-Augmented     │\n",
              "│ Generation (RAG), which is a semantic search engine that uses vectorial representations of document sections to │\n",
              "│ determine the meaning stored in each section. Fine-tuned LLM can be used to handle specific jargon and          │\n",
              "│ incorporate it into the RAG architecture to reap the combined benefits. The main challenges with setting up a   │\n",
              "│ successful RAG solution include ensuring the quality of the generative AI, attracting and retaining a large     │\n",
              "│ number of users, managing the traction of the system, and balancing the benefits of RAG with the potential for  │\n",
              "│ market growth. Proper setup takes time and attention, and selecting the right LLM is important for cost and     │\n",
              "│ data management. LLMOps can support government staff in answering parliamentary questions more easily, with     │\n",
              "│ specific knowledge of a moderate database of knowledge articles that contain useful information that is not     │\n",
              "│ easily found on the world-wide web.                                                                             │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:05:02\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m822489\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:05:02.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">822489</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using LangChain CHAIN"
      ],
      "metadata": {
        "id": "K45BGKhBfniS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, T5Tokenizer, T5Model\n",
        "from transformers import T5ForConditionalGeneration, pipeline\n",
        "import torch\n",
        "import rich\n",
        "from rich.console import Console\n",
        "from rich.panel import Panel\n",
        "from rich import print\n",
        "import ssl\n",
        "import requests\n",
        "import datetime\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import Image, display\n",
        "from tqdm import tqdm\n",
        "\n",
        "########### IMPORT HFACE PIPELINE FROM LANGCHAIN AND SUMMARIZE CHAIN ################\n",
        "from langchain.llms import HuggingFacePipeline \n",
        "from langchain.chains.summarize import load_summarize_chain \n",
        "\n",
        "############### DEFINING THE LAMINI MODEL CHACKPOINT IN LANGCHAIN #################\n",
        "checkpoint = \"/content/model/\"  #it is actually LaMini-Flan-T5-248M   #tested fine\n",
        "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(checkpoint,\n",
        "                                                    device_map='auto',\n",
        "                                                    torch_dtype=torch.float32)\n",
        "\n",
        "################## HUGGINGFACEPIPELINE DETAILS  ###################\n",
        "# To use, you should have the transformers python package installed.\n",
        "# Only supports text-generation, text2text-generation and summarization for now.\n",
        "llm = HuggingFacePipeline.from_model_id(model_id=checkpoint, \n",
        "                                        task = 'text2text-generation', \n",
        "                                        model_kwargs={\"temperature\":0, \"max_length\":512})\n",
        "\n",
        "########## INITIALIZE RICH CONSOLE  ##################\n",
        "console = Console()\n",
        "\n",
        "################## PREPARE WIDGET #########################\n",
        "testo = widgets.Textarea(\n",
        "    value='',\n",
        "    placeholder=f'Paste here the text you want to summarize',\n",
        "    description=f'TEXT:',\n",
        "    disabled=False,\n",
        "    rows = 20,   \n",
        "    layout = widgets.Layout( width='75%')\n",
        ")\n",
        "display(testo)"
      ],
      "metadata": {
        "id": "LkJSfonxEEHP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "714b9fee46ba44e58e1ad1077bff6d24",
            "ea3a996bd5964e638eb55bb23b3161dd",
            "b339040d015a4eaf8d9abd09d092ab27"
          ]
        },
        "outputId": "d315fe21-0473-4a1e-9b1f-e54d07c6125a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Textarea(value='', description='TEXT:', layout=Layout(width='75%'), placeholder='Paste here the text you want …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "714b9fee46ba44e58e1ad1077bff6d24"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## TEXT SPLITTER FUNCTION #################\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 460,\n",
        "    chunk_overlap  = 40,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.create_documents([testo.value])\n",
        "\n",
        "########### RUN THE SUMMARIZE CHAIN ######################\n",
        "\n",
        "summaries = []\n",
        "start = datetime.datetime.now() #not used now but useful\n",
        "console.print(\"[yellow bold] Inizializing Summarize Chain\")\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "with console.status('Running Summarization chain...', spinner='point'):\n",
        "  summary = chain.run(texts)\n",
        "  console.print(\"Summary for: User input text\")\n",
        "  console.print(f\"with LaMini-Flan-T5-248M and LangChain \\n\")\n",
        "  summaries.append(summary)\n",
        "  stop = datetime.datetime.now() #not used now but useful  \n",
        "  delta = stop-start  \n",
        "  print(Panel(summary, title='AI Summarization'))  \n",
        "  console.print(f\"[red bold]Summarization completed in {delta}\")\n"
      ],
      "metadata": {
        "id": "YIXm_OsIVGTj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310,
          "referenced_widgets": [
            "de7d220717b54acf92341ca78435f466",
            "b760a15e2bca4d78aaf5148ea6c695ce"
          ]
        },
        "outputId": "e8640fca-62c9-4892-aeb2-acef2041663d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de7d220717b54acf92341ca78435f466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1808 > 1024). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1860 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summary for: User input text\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Summary for: User input text\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "with LaMini-Flan-T5-248M and LangChain \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">with LaMini-Flan-T5-248M and LangChain \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses the lack of mindlessly scaling models in AI and the importance of understanding how to    │\n",
              "│ scale models before scaling them. The article also mentions the hype surrounding certain models and suggests    │\n",
              "│ that it's time to get into the subject of image source saturation of benchmarks. The article also mentions the  │\n",
              "│ importance of data processing and curation in the pipeline, and suggests that people should spend more time on  │\n",
              "│ data. The article also mentions the streetlight effect, a type of observational bias where people focus on      │\n",
              "│ funding or copying what is already being done, resulting in a mindless scaling model size without attention     │\n",
              "│ given to other factors. The article concludes by highlighting the need for more scaling and highlighting the    │\n",
              "│ importance of building better models and improving data transformation processes.                               │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────── AI Summarization ────────────────────────────────────────────────╮\n",
              "│ The article discusses the lack of mindlessly scaling models in AI and the importance of understanding how to    │\n",
              "│ scale models before scaling them. The article also mentions the hype surrounding certain models and suggests    │\n",
              "│ that it's time to get into the subject of image source saturation of benchmarks. The article also mentions the  │\n",
              "│ importance of data processing and curation in the pipeline, and suggests that people should spend more time on  │\n",
              "│ data. The article also mentions the streetlight effect, a type of observational bias where people focus on      │\n",
              "│ funding or copying what is already being done, resulting in a mindless scaling model size without attention     │\n",
              "│ given to other factors. The article concludes by highlighting the need for more scaling and highlighting the    │\n",
              "│ importance of building better models and improving data transformation processes.                               │\n",
              "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mSummarization completed in \u001b[0m\u001b[1;31m0:04:12\u001b[0m\u001b[1;31m.\u001b[0m\u001b[1;31m744316\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Summarization completed in 0:04:12.</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">744316</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The 🥇 *text2text* generation pipeline with   🦜🔗 LangChain win both in terms of time ⏲ and in terms of 👍 quality"
      ],
      "metadata": {
        "id": "KBI463bjwFrT"
      }
    }
  ]
}